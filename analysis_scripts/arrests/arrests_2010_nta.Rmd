---
title: "Arrests 2010 NTA: Analisi"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
```

```{r, message=FALSE, warning=FALSE}
rm(list = ls())
gc()

source("functions_script.R")
```

```{r}
# Condition: if TRUE execute the computational heavy chuncks, else not
# assuming the models are been loaded from a RData file before.
# IMPORTANT: in order to reproduce the result set bool_execute_heavy_chunks = TRUE

bool_execute_heavy_chunks = TRUE

file_save_name = "arrests_nta_2010_models_summary.Rdata"

# require loading models_summary list
if(!bool_execute_heavy_chunks){
  load(file_save_name)
} else{
  # store each model useful information:
  # validation error and beta for best lambda
  models_summary = list()
}
```


```{r}
# Preprocessing -------------------------------------
# read the data
my.df = read.csv("../../data/final_datasets/arrests_2010_nta.csv", stringsAsFactors = T)
```

Remove date and NTA variables, convert to factor location, month and KY_CD.
```{r, message=FALSE}
my.df$ARREST_DATE = NULL
my.df$YEAR = NULL
my.df$Latitude = NULL
my.df$Longitude = NULL

my.df$NTA2020 = factor(my.df$NTA2020)
my.df$MONTH = factor(my.df$MONTH)

# add NA category
my.df$KY_CD = ifelse(is.na(my.df$KY_CD), "MISSING", my.df$KY_CD)
my.df$KY_CD = factor(my.df$KY_CD)

str(my.df)

# check for NA
apply(my.df, 2, function(col) (sum(is.na(col))))

# the ratio is high
nrow(na.omit(my.df)) / nrow(my.df)

# delete missing values
my.df = na.omit(my.df)
```


## Descrizione

L'obbiettivo di questa sezione di analisi è verificare se esiste un sottoinsieme di variabili esplicative particolarmente correlate con il numero di arresti, sia marginalmente che considerando l'interazione con ciascuna zona spaziale (NTA).
Per vincoli computazionali si riduce l'insieme di stima al solo anno 2010: per quest'anno i dati del censo sono esatti e non si sono verificati eventi rari a differenza del 2020 (Covid); l'insieme di verifica scelto è l'anno 2011, in quanto è l'anno più vicino al 2010 (l'assunzione è che i due anni siano abbastanza simili per il fenomeno considerato).

### Variabili considerate

Viene riportata la lista delle variabili considerate con una breve descrizione.

Le variabili considerate dal dataset arrests sono:

- KY_CD (fattore con 70 livelli): categoria granulare del crimine causa dell'arresto
- LAW_CAT_CD (fattore con 5 livelli): categoriea generale del crimine causa dell'arresto
- AGE_GROUP (fattore con 5 livelli): classe d'età dell'arrestato
- PERP_SEX  (fattore con 2 livelli): sesso dell'arrestato
- PERP_RACE (fattore con 7 livelli): etnia dell'arrestato
- NTA2020 (fattore con 251 livelli): indicatore della specifica NTA del luogo dell'arresto
- MONTH (fattore con 12 livelli): indicatore del mese dell'arresto

Le variabili considerate dal censo sono:
- Pop1 (numerica): popolazione per NTA
- MaleP (numerica): percentuale di maschi per NTA
- MdAge (numerica): età mediana per NTA
- Hsp1P (numerica): percentuale di ispanici per NTA
- WNHP (numerica): percentuale di bianchi non ispanici (NH) per NTA
- BNHP (numerica): percentuale di neri non ispanici per NTA
- ANHP (numerica): percentuale di asiatici non ispanici per NTA
- OthNHP (numerica): percentuale di altre etnie non ispaniche per NTA
- MIncome (numerica): reddito mediano per NTA


### Problematiche

Questi dati presentano diverse problematiche.

Le scelte fatte sono dovute a fattori computazionali, di tempo e al fatto che per permettere conteggi diversi da 1 è necessario considerare zone spaziali e intervalli temporali non eccessivamente ristretti.

Per selezione delle variabili di stratificazione per zona da includere, benchè il censo fornisca molte variabili si è scelto di considerarne solo un esiguo sottoinsieme: ciò è dovuto a una mancanza di conoscenza di campo e al non poter dedicare eccessivo tempo alla selezione ed in particolare all'estrazione delle stesse dai varia database e fogli di calcolo.

Per la definizione delle zone spaziali e degli intervalli temporali.
Inizialmente si era pensato di eseguire le analisi sull'unità spaziale più piccola disponibile per cui sono presenti i dati del censo: i "Census Tracts" (CT) che per New York sono più di 2000 zone distinte (a differenza delle 250 degli NTA), si è però subito verificato che con i mezzi a disposizione lavorare su modelli con i CT era improponibile a causa delle eccessive risorse computazionali richieste. Per quanto concerne gli intervalli temporali si è scelto di ignorare i possibili trend e considerare i singoli anni, per ciascun anno si sono utilizzati i mesi per la costruzione degli insiemi di convalida incrociata. Pur avendo a disposizione il giorno di ciascun arresto la selezione dei mesi è apparsa come un giusto compromesso per garantire dei conteggi superiori a 1.

Pur avendo notevolmente ridotto la potenziale complessità del problema il numero di osservazioni e il numero di modalità dei fattori alcune procedure impiegate possono comunque molto tempo (sulle piattaforme di calcolo impiegate) per essere eseguite: questi limiti rendono proibitive valutazioni dell'incertezza nelle stime, quali metodi bootstrap o bayesiani.

Poichè i conteggi sono costruiti raggruppando le osservazioni con le stesse combinazioni di modalità delle covariate, il conteggio minimo osservabile è uguale a 1.
Ciò pone dei possibili problemi nella stima di modelli che comprendono lo zero nel supporto (ad esempio il modello di Poisson). Si è comunque provato ad adattare modelli (sia a risposta continua che discreta) su questa tipologia di dati, una delle possibili soluzioni al di fuori di impiegare modelli diversi è la metodologia "zero padding" in cui per ogni combinazione di variabili per cui non si verificano casi si aggiunge un'osservazione con conteggio nullo: la criticità qui è computazionale dovuta all'incremento notevole delle osservazioni.


```{r}
year_grouped = suppressMessages(my.df %>%
  dplyr::select(-"MONTH") %>% 
  group_by_all() %>% 
  summarise(count = n()))

```

Still a huge number of observations compared to the number of variables, but what if we add interactions?


A sinistra i conteggi di arresti raggruppati per valori di covariate e a destra il logaritmo della medesima quantità
```{r}
par(mfrow = c(1,2))

year_grouped$count %>%
  table %>% 
  plot(main = "Arrests ratio grouped by covariates",
       xlab = "number of arrests",
       ylab = "absolute frequency")

year_grouped$count %>% 
  log %>% 
  table %>% 
  plot(main = " log arrests count grouped by covariates",
       xlab = "log(arrests count)",
       ylab = "absolute frequency")

par(mfrow = c(1,1))
```



```{r}
# to include zeros
# cols_to_complete = colnames(my.df)
# cols_to_complete = setdiff(cols_to_complete, "MONTH")
# 
# # Create a data frame with all combinations of the columns
# all_combinations <- expand_grid(!!!syms(cols_to_complete))
# 
# # Summarize the data and complete with zeros
# year_grouped_zeros <- my.df %>%
#   select(-MONTH) %>%
#   group_by(across(all_of(cols_to_complete))) %>%
#   summarise(count = n(), .groups = 'drop') %>%
#   ungroup() %>%
#   right_join(all_combinations, by = cols_to_complete) %>%
#   replace_na(list(count = 0))
# 
# print(year_grouped_zeros)

```

### Elevata dimensionalità

I dati presentano elevata dimensionalità considerando le interazioni tra la variabile spaziale (NTA) e le covariate (qualitative) di arrests.
E' comunque interessante provare i metodi di selezione delle variabili anche sui dati senza interazioni.

Per avere delle misure quantitative si considera il dataset in cui si sono definiti i conteggi senza considerare i mesi: si riporta il rapporto tra il numero di osservazioni (righe) e il prodotto tra il numero di modalità di NTA e la somma delle modalità delle variabili qualitative di arrests.

```{r}
var_unique_len = apply(year_grouped,
                       2,
                       function(col) length(unique(col)))
```

Senza considerare KY_CD (esplicative non spaziale di arrest con più modalità) il rapporto è:
```{r, include = TRUE}
nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("LAW_CAT_CD", "AGE_GROUP", "PERP_SEX", "PERP_RACE")]))
```

Considerando anche KY_CD il rapporto è:
```{r, include=TRUE}
nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("KY_CD", "LAW_CAT_CD", "AGE_GROUP",
                       "PERP_SEX", "PERP_RACE")]))
```


Poichè dopo alcune prove le procedure si sono rivelate troppo onerose includendo KY_CD si è deciso di rimuoverla sperando che l'informazione fornita da LAW_CAT_CD sia sufficiente.
```{r}
my.df$KY_CD = NULL
```

## Modelli

### Criterio di selezione dei parametri di regolazione

Come già accennato, i parametri di regolazione sono selezionati tramite convalida incrociata (CV) impiegando i mesi per la costruzione degli insiemi.

La procedura per la costruzione degli insiemi è la seguente:
- Selezione di k: il numero di insiemi di convalida (ad esempio k = 4)
- Ogni insieme di convalida è composto da osservazioni raggruppate di 12 / k (3) mesi e i mesi rimanenti (9) vengono utilizzati per adattare il modello.
- Per cercare di compensare e mediare le fluttuazioni stagionali, i mesi di validazione sono scelti il più distanziati possibile. Ad esempio, nel caso di k = 4, il primo insieme di validazione è (gennaio, maggio, settembre), il secondo set è (febbraio, giugno, ottobre), il terzo è (marzo, luglio, novembre) e il quarto è (aprile, agosto, dicembre).
- Per rendere ogni risposta comparabile avendo utilizzato un numero diverso di mesi, una nuova risposta è definita come il rapporto degli arresti diviso per il numero di mesi utilizzati nel raggruppamento (ovvero l'esponziale dell 'offset nel modello di Poisson).

```{r}
# months indexes sets
# each list contains a matrix where each row contains the used indexes
month_sets_ind = list(k4 = matrix(c(1, 5, 9,
                                    2, 6, 10,
                                    3, 7, 11,
                                    4, 8, 12),
                                  byrow = T,
                                  nrow = 4),
                      
                      k6 = matrix(c(1, 7,
                                    2, 8,
                                    3, 9,
                                    4, 10,
                                    5, 11,
                                    6, 12),
                                  byrow = T,
                                  nrow = 6))
```

### Modelli Continui

when using a linear model (assuming gaussian errors) we consider the response as: 
$y = \log(\text{count} / \text{population})$ where each count is the events count obtained by grouping by all other covariates and each population is specific to each NTA.


### Matrice del modello


```{r}

FORMULA.YES.INTERACTIONS = as.formula("count ~. -1 + 
                            NTA2020:LAW_CAT_CD")

FORMULA.NO.INTERACTIONS = as.formula("count ~. -1")
```




```{r, warning=FALSE}

my.df.year.grouped = suppressMessages(my.df %>%
  dplyr::select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n()))

```

```{r}
my.df.year.grouped.matrix = sparse.model.matrix(FORMULA.YES.INTERACTIONS,
                                                my.df.year.grouped)
```

```{r}
# NO interaction
month.noint.cv.k4.sets = suppressMessages(MakeMonthCvSets(my_df = my.df,
                       month_sets_ind = month_sets_ind$k4,
                       formula = FORMULA.NO.INTERACTIONS))
# Yes interaction
month.yesint.cv.k4.sets = suppressMessages(MakeMonthCvSets(my_df = my.df,
                       month_sets_ind = month_sets_ind$k4,
                       formula = FORMULA.YES.INTERACTIONS))
```



### Note on quantitative covariates

The simplest assumption is to assume a linear (monotone) trend of the response as a function of quantitative covariates.

### LASSO

```{r, eval = bool_execute_heavy_chunks}
models_summary$lasso.noint <- Lasso_CV(month.noint.cv.k4.sets,
                                        my.lambda.vals = exp(seq(-17, -6, 0.1)) %>% sort(decreasing = T))
```

```{r, eval = bool_execute_heavy_chunks}
models_summary$lasso <- Lasso_CV(month.yesint.cv.k4.sets,
                                        my.lambda.vals = exp(seq(-13, -6, 0.1)) %>% sort(decreasing = T))
```

```{r}
par(mfrow = c(1,2))

PlotOneDim(x = log(models_summary$lasso.noint$lambda),
           y = models_summary$lasso.noint$cv.err.matr[,"cv.err"],
           se = models_summary$lasso.noint$cv.err.matr[,"cv.se"],
           x.min = log(models_summary$lasso.noint$lmin),
           x.1se = log(models_summary$lasso.noint$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO no interactions CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

PlotOneDim(x = log(models_summary$lasso$lambda),
           y = models_summary$lasso$cv.err.matr$cv.err,
           se = models_summary$lasso$cv.err.matr$cv.se,
           x.min = log(models_summary$lasso$lmin),
           x.1se = log(models_summary$lasso$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO yes interaction CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

par(mfrow = c(1,1))
```

### Elasticnet


```{r,eval = bool_execute_heavy_chunks}
models_summary$elasticnet.noint = Elastic_Cv(cv.sets = month.noint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-20, -8, 0.1)) %>% sort(decreasing = TRUE),
                                             my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20))
```

```{r, eval = bool_execute_heavy_chunks}

models_summary$elasticnet = Elastic_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-20, -8, 0.1)) %>% sort(decreasing = TRUE),
                                             my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20))
```

```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$elasticnet.noint,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

TwoParErrPlot(model_list = models_summary$elasticnet,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

par(mfrow = c(1,1))
```

### Grouped LASSO


Pur avendo provato a considerare solo il modello senza interazioni, a causa dell'eccessivo tempo computazionale richiesto non è qui riportato.
```{r}
A = colnames(month.noint.cv.k4.sets[[1]]$train$df)
# Create a named vector to map prefixes to values
prefix_to_value <- setNames(seq_along(A), A) 

B = colnames(month.noint.cv.k4.sets[[1]]$train$model_matrix)

# Create indexes vector
var_indexes <- sapply(B, function(item) {
  prefix <- A[sapply(A, function(prefix) startsWith(item, prefix))]
  prefix_to_value[prefix]
})

```


```{r}
# models_summary$glasso = GLasso_CV(month.noint.cv.k4.sets,
#                                         my.lambda.vals = exp(seq(-17, -6, 0.1)) %>% sort(decreasing = T),
#                                   group_index = var_indexes)
```


### Scad

```{r, eval = bool_execute_heavy_chunks}
models_summary$scad.noint = NonConvex_Cv(cv.sets = month.noint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-5, 15, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 10, length = 10),
                                         my.penalty = "scad")
```

```{r, warning = FALSE, eval = bool_execute_heavy_chunks}
models_summary$scad = NonConvex_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-1, 10, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 10, length = 10),
                                         my.penalty = "scad")

```

```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$scad.noint,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "SCAD no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

TwoParErrPlot(model_list = models_summary$scad,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "SCAD yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

par(mfrow = c(1,1))

```

```{r,  warning=FALSE, eval = bool_execute_heavy_chunks}
# scad.fit.all = ncvreg(X = as.matrix(my.df.year.grouped.matrix),
#                  y = my.df.year.grouped$y,
#                  gamma = models_summary$scad$gmin,
#                  penalty = "scad")
# 
# plot(scad.fit.all, log.l = TRUE)
# abline(v = log(models_summary$scad$lmin), col = "blue", lty = 2, lwd = 2)

```


### MCP

```{r, eval = bool_execute_heavy_chunks}
models_summary$mcp.noint = NonConvex_Cv(cv.sets = month.noint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-5, 5, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 10, length = 10),
                                         my.penalty = "mcp")
```

```{r, warning = FALSE, eval = bool_execute_heavy_chunks}
models_summary$mcp = NonConvex_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-5, 5, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 10, length = 10),
                                         my.penalty = "mcp")
```


```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$mcp.noint,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "MCP no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

TwoParErrPlot(model_list = models_summary$mcp,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "MCP yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

par(mfrow = c(1,1))
```

```{r, eval = bool_execute_heavy_chunks}
# mcp.fit.all = ncvreg(X = as.matrix(my.df.year.grouped.matrix),
#                  y = my.df.year.grouped$y,
#                  gamma = models_summary$mcp$gmin,
#                  penalty = "mcp")
# 
# plot(mcp.fit.all, log.l = TRUE)
# abline(v = log(models_summary$mcp$gmin), col = "blue", lty = 2, lwd = 2)

```

```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```


### Discrete response models

In reality the counts are discrete so it seems reasonable to also try discrete response models such as Poisson, Negative Binomial and zero inflated Poisson.
For all such cases, using the counts as response an offset has to be imposed: in analogy from what has been done assuming the continuous response the offset will be the product of the NTA Population by the number of months considered (in log scale using the canonincal log link for a Poisson GLM)

### Poisson LASSO

```{r}
# Known convergence difficult, see help here. https://cran.r-project.org/web/packages/glmnet/vignettes/glmnetFamily.pdf
glmnet.control(mxitnr = 50) # increase maximum no. of IRLS iterations allowed
```

```{r, eval = bool_execute_heavy_chunks, warning=FALSE}
models_summary$lasso.poi.noint = Lasso_Offset_CV(cv.sets = month.noint.cv.k4.sets,
                                          my.lambda.vals = exp(seq(-15, 3, 0.1)) %>% sort(decreasing = TRUE),
                                          my.family = poisson())
```

```{r, eval = bool_execute_heavy_chunks, warning=FALSE}
models_summary$lasso.poi = Lasso_Offset_CV(cv.sets = month.yesint.cv.k4.sets,
                                          my.lambda.vals = exp(seq(-15, 3, 0.1)) %>% sort(decreasing = TRUE),
                                          my.family = poisson())
```

```{r}
par(mfrow = c(1,2))
PlotOneDim(x = log(models_summary$lasso.poi.noint$lambda),
           y = models_summary$lasso.poi.noint$cv.err.matr[,"cv.err"],
           se = models_summary$lasso.poi.noint$cv.err.matr[,"cv.se"],
           x.min = log(models_summary$lasso.poi.noint$lmin),
           x.1se = log(models_summary$lasso.poi.noint$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO no interaction Poisson CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

PlotOneDim(x = log(models_summary$lasso.poi$lambda),
           y = models_summary$lasso.poi$cv.err.matr$cv.err,
           se = models_summary$lasso.poi$cv.err.matr$cv.se,
           x.min = log(models_summary$lasso.poi$lmin),
           x.1se = log(models_summary$lasso.poi$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO Poisson yes interaction CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

par(mfrow = c(1,1))
```


```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```

### Poisson Elasticnet

```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$elasticnet.poi.noint = Elastic_Offset_Cv(cv.sets = month.noint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20),
                                                        my.family = poisson())
```

```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$elasticnet.poi = Elastic_Offset_Cv(cv.sets = month.yesint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20),
                                                        my.family = poisson())
```

```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$elasticnet.poi.noint,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

TwoParErrPlot(model_list = models_summary$elasticnet.poi,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet Poisson yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

par(mfrow = c(1,1))
```

```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```

### Neg bin Lasso

Using a hierarchical specification we assume $Y_i \sim P(\mu_i \lambda_i)$ and $\lambda_i \sim Ga(\tau, \tau)$ so marginally the $Y_i$ are negative binomials with variance $\mu_i(1 + \tau \mu_i)$.
In the R parameterization adopted $\theta = 1/\tau$ which becomes another tuning parameter.


```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$lasso.negbin.noint = LASSO_NegBin_Offset_Cv(cv.sets = month.noint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-3, 5, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.theta.vals = seq(1e-5, 3, length = 20))
```

```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$lasso.negbin = LASSO_NegBin_Offset_Cv(cv.sets = month.yesint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.theta.vals = seq(1e-5, 20, length = 20))
```

```{r, eval = bool_execute_heavy_chunks}
par(mfrow = c(1,2))

TwoParErrPlot(model_list = models_summary$lasso.negbin.noint,
              row_par_name = "lambda",
              col_par_name = "theta",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "tmin",
              my.main = "LASSO Negative Binomial no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "theta")

TwoParErrPlot(model_list = models_summary$lasso.negbin,
              row_par_name = "lambda",
              col_par_name = "theta",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "tmin",
              my.main = "LASSO Negative Binomial yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "theta")

par(mfrow = c(1,1))
```

```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```

### Modelli migliori












