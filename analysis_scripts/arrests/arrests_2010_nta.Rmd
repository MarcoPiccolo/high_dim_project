---
title: "Arrests 2010 NTA analysis"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
```

```{r, message=FALSE, warning=FALSE}
rm(list = ls())
gc()

source("functions_script.R")
```

```{r}
# Condition: if TRUE execute the computational heavy chuncks, else not
# assuming the models are been loaded from a RData file before.
# IMPORTANT: in order to reproduce the result set bool_execute_heavy_chunks = TRUE

bool_execute_heavy_chunks = FALSE

file_save_name = "arrests_nta_2010_models_summary.Rdata"

# require loading models_summary list
if(!bool_execute_heavy_chunks){
  load(file_save_name)
} else{
  # store each model useful information:
  # validation error and beta for best lambda
  models_summary = list()
}
```

## Preprocessing
```{r}
# read the data
my.df = read.csv("../../data/final_datasets/arrests_2010_nta.csv", stringsAsFactors = T)
```

Remove date and NTA variables, convert to factor location, month and KY_CD.
```{r}
my.df$ARREST_DATE = NULL
my.df$YEAR = NULL
my.df$Latitude = NULL
my.df$Longitude = NULL

my.df$NTA2020 = factor(my.df$NTA2020)
my.df$MONTH = factor(my.df$MONTH)

# add NA category
my.df$KY_CD = ifelse(is.na(my.df$KY_CD), "MISSING", my.df$KY_CD)
my.df$KY_CD = factor(my.df$KY_CD)
```


```{r}
str(my.df)
```

Check for NA
```{r}
apply(my.df, 2, function(col) (sum(is.na(col))))
```


A possibility is to get rid of all NAs rows, the portion of deleted rows would be relatively small (of course we're introducing some bias here).
```{r}
nrow(na.omit(my.df)) / nrow(my.df)
```
Other possibilities would be to impute values for numerical variables (using median, mean or more sofisticated methods).
For simplicity we just delete missing values rows.
```{r}
my.df = na.omit(my.df)
```


## Description

Ideally 2010 data are our training set and 2011 data are the test set.
The goal of the analysis is to identify is some covariates are correlated with the arrests rate: more specifically if the response is well explained by some non spatial covariates alone, some spatial alone or interaction between the two.

A reasonable response variable would be the count of arrests divided by the local (space zone) population, also grouping by any other covariates value.



To get an idea of the dataset used on which models are tested a
```{r}
year_grouped = my.df %>%
  dplyr::select(-"MONTH") %>% 
  group_by_all() %>% 
  summarise(count = n())

# define the arrest rate (count / population)
year_grouped$y =  year_grouped$count / year_grouped$Pop1

```

```{r}
dim(year_grouped)
colnames(year_grouped)
```
Still a huge number of observations compared to the number of variables, but what if we add interactions?

Let's look at the distribution of the counts.
```{r}
year_grouped$count %>%
  table %>% 
  plot(main = "Arrests counts grouped by all covariates observed combinations",
       xlab = "arrest count",
       ylab = "absolute frequency")
```
We can see an inflation of ones.
The ratios present a similar frequency table. Taking the logarithm of the ratio the distribution is still (a bit less) skewed.
```{r}
par(mfrow = c(1,2))

year_grouped$y %>%
  table %>% 
  plot(main = "Arrests ratio grouped by covariates",
       xlab = "number of arrests / nta population",
       ylab = "absolute frequency")

year_grouped$y %>% 
  log %>% 
  table %>% 
  plot(main = " log arrests ratio grouped by covariates",
       xlab = "log(number of arrests / nta population)",
       ylab = "absolute frequency")

par(mfrow = c(1,1))
```



```{r}
# to include zeros
# cols_to_complete = colnames(my.df)
# cols_to_complete = setdiff(cols_to_complete, "MONTH")
# 
# # Create a data frame with all combinations of the columns
# all_combinations <- expand_grid(!!!syms(cols_to_complete))
# 
# # Summarize the data and complete with zeros
# year_grouped_zeros <- my.df %>%
#   select(-MONTH) %>%
#   group_by(across(all_of(cols_to_complete))) %>%
#   summarise(count = n(), .groups = 'drop') %>%
#   ungroup() %>%
#   right_join(all_combinations, by = cols_to_complete) %>%
#   replace_na(list(count = 0))
# 
# print(year_grouped_zeros)

```


Let's count the hypothetical number of interaction terms if ones considers only interactions between spatial zones and selected arrests covariates along with the obervations /  number of parameter ratio (underestimate since there are other variables):
```{r}
var_unique_len = apply(year_grouped,
                       2,
                       function(col) length(unique(col)))

var_unique_len
```

Not including KY_CD:
```{r}
var_unique_len["NTA2020"] *
  sum(var_unique_len[c("LAW_CAT_CD", "AGE_GROUP", "PERP_SEX", "PERP_RACE")])

nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("LAW_CAT_CD", "AGE_GROUP", "PERP_SEX", "PERP_RACE")]))
```

Including KY_CD
```{r}
var_unique_len["NTA2020"] *
  sum(var_unique_len[c("KY_CD","LAW_CAT_CD", "AGE_GROUP",
                       "PERP_SEX", "PERP_RACE")])

nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("KY_CD", "LAW_CAT_CD", "AGE_GROUP",
                       "PERP_SEX", "PERP_RACE")]))
```
We decide to not employ the MONTH time variable as a covariate but use it for a model selection method.
This is because in order to get a count measure one aggregation criterion is needed, here aggregation by month is chosen.


### Variables description

Original dataset selected variables:

Census stratification variables:


## Explorative analysis

### Arrests count vs month
```{r}
my.df %>% 
  group_by_at(vars(MONTH)) %>%
  summarise(count = n()) %>% 
  plot(main = "Arrests count by month")
```


### Arrests counts vs NTA

### Arrest counts vs other covariates

## Models

### Model selection method

Given the previously described constraints, in order to be able to apply a cross validation (CV) selection method we choose to ignore the time (MONTH) factor using MONTH as index to create the CV folds as described below.
Choose k: the number of validation sets (example k = 4) each validation set is made by grouped observations of 12 / k (3) months and the months left (9) are used to fit the model. To try to compensate and average for seasonal fluctuations the validation months are chosen as spaced as possible, for example, in the case k = 4 the first validation set is (january, may, september), the second set is (february, june, october), the third is (march, july, november) and the forth is (april, august, december); in order to make each response comparable having used a different number of months a new response is defined as the arrests ratio divided by the number of months used in the grouping. Note: this is also a way to reduce the computational burden compared to using many more months combinations.

```{r}
# Groub by months indexes
GroupByMONTHSets = function(mydf, month_indexes){
  month_grouped = mydf %>%
    filter(MONTH %in% month_indexes) %>% 
    dplyr::select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n())
  
  return(month_grouped)
}
```

Define Month indexes
```{r}
# months indexes sets
# each list contains a matrix where each row contains the used indexes
month_sets_ind = list(k4 = matrix(c(1, 5, 9,
                                    2, 6, 10,
                                    3, 7, 11,
                                    4, 8, 12),
                                  byrow = T,
                                  nrow = 4),
                      
                      k6 = matrix(c(1, 7,
                                    2, 8,
                                    3, 9,
                                    4, 10,
                                    5, 11,
                                    6, 12),
                                  byrow = T,
                                  nrow = 6))
```

In order to simplify computations we remove the KY_CD variable (hoping LAW_CAT_CD will be sufficient to describe the crime type)
when using a linear model (assuming gaussian errors) we consider the response as: 
$y = \log(\text{count} / \text{population})$ where each count is the events count obtained by grouping by all other covariates and each population is specific to each NTA.
```{r, warning=FALSE}
my.df$KY_CD = NULL

my.df.year.grouped = my.df %>%
  dplyr::select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n())


# take the log to 
my.df.year.grouped$y = log(my.df.year.grouped$count) - log(my.df.year.grouped$Pop1 + 1) - log(12)
```

```{r}

FORMULA.YES.INTERACTIONS = as.formula("count ~. -1 + 
                            NTA2020:LAW_CAT_CD")

FORMULA.NO.INTERACTIONS = as.formula("count ~. -1")
```


```{r}
my.df.year.grouped.matrix = sparse.model.matrix(FORMULA.YES.INTERACTIONS,
                                                my.df.year.grouped)
```


```{r}
# NO interaction
month.noint.cv.k4.sets = suppressMessages(MakeMonthCvSets(my_df = my.df,
                       month_sets_ind = month_sets_ind$k4,
                       formula = FORMULA.NO.INTERACTIONS))
# Yes interaction
month.yesint.cv.k4.sets = suppressMessages(MakeMonthCvSets(my_df = my.df,
                       month_sets_ind = month_sets_ind$k4,
                       formula = FORMULA.YES.INTERACTIONS))
```


```{r}
# error functions
RMSEfun = function(true_vals, pred_vals){
  sqrt(mean( (true_vals - pred_vals)^2 ))
}
```


### Note on quantitative covariates

The simplest assumption is to assume a linear (monotone) trend of the response as a function of quantitative covariates.

### LASSO

```{r, eval = bool_execute_heavy_chunks}
models_summary$lasso.noint <- Lasso_CV(month.noint.cv.k4.sets,
                                        my.lambda.vals = exp(seq(-17, -6, 0.1)) %>% sort(decreasing = T))
```

```{r, eval = bool_execute_heavy_chunks}
models_summary$lasso <- Lasso_CV(month.yesint.cv.k4.sets,
                                        my.lambda.vals = exp(seq(-13, -6, 0.1)) %>% sort(decreasing = T))
```

```{r}
par(mfrow = c(1,2))

PlotOneDim(x = log(models_summary$lasso.noint$lambda),
           y = models_summary$lasso.noint$cv.err.matr[,"cv.err"],
           se = models_summary$lasso.noint$cv.err.matr[,"cv.se"],
           x.min = log(models_summary$lasso.noint$lmin),
           x.1se = log(models_summary$lasso.noint$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO no interactions CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

PlotOneDim(x = log(models_summary$lasso$lambda),
           y = models_summary$lasso$cv.err.matr$cv.err,
           se = models_summary$lasso$cv.err.matr$cv.se,
           x.min = log(models_summary$lasso$lmin),
           x.1se = log(models_summary$lasso$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO yes interaction CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

par(mfrow = c(1,1))
```

### Elasticnet


```{r,eval = bool_execute_heavy_chunks}
models_summary$elasticnet.noint = Elastic_Cv(cv.sets = month.noint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-20, -8, 0.1)) %>% sort(decreasing = TRUE),
                                             my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20))
```

```{r, eval = bool_execute_heavy_chunks}

models_summary$elasticnet = Elastic_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-20, -8, 0.1)) %>% sort(decreasing = TRUE),
                                             my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20))
```

```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$elasticnet.noint,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

TwoParErrPlot(model_list = models_summary$elasticnet,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

par(mfrow = c(1,1))
```

### Grouped LASSO


Pur avendo provato a considerare solo il modello senza interazioni, a causa dell'eccessivo tempo computazionale richiesto non è qui riportato.
```{r}
A = colnames(month.noint.cv.k4.sets[[1]]$train$df)
# Create a named vector to map prefixes to values
prefix_to_value <- setNames(seq_along(A), A) 

B = colnames(month.noint.cv.k4.sets[[1]]$train$model_matrix)

# Create indexes vector
var_indexes <- sapply(B, function(item) {
  prefix <- A[sapply(A, function(prefix) startsWith(item, prefix))]
  prefix_to_value[prefix]
})

```


```{r}
# models_summary$glasso = GLasso_CV(month.noint.cv.k4.sets,
#                                         my.lambda.vals = exp(seq(-17, -6, 0.1)) %>% sort(decreasing = T),
#                                   group_index = var_indexes)
```


### Scad

```{r, eval = bool_execute_heavy_chunks}
models_summary$scad.noint = NonConvex_Cv(cv.sets = month.noint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-5, 15, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 10, length = 10),
                                         my.penalty = "scad")
```

```{r, warning = FALSE, eval = bool_execute_heavy_chunks}
models_summary$scad = NonConvex_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-1, 10, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 10, length = 10),
                                         my.penalty = "scad")

```

```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$scad.noint,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "SCAD no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

TwoParErrPlot(model_list = models_summary$scad,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "SCAD yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

par(mfrow = c(1,1))

```

```{r,  warning=FALSE, eval = bool_execute_heavy_chunks}
# scad.fit.all = ncvreg(X = as.matrix(my.df.year.grouped.matrix),
#                  y = my.df.year.grouped$y,
#                  gamma = models_summary$scad$gmin,
#                  penalty = "scad")
# 
# plot(scad.fit.all, log.l = TRUE)
# abline(v = log(models_summary$scad$lmin), col = "blue", lty = 2, lwd = 2)

```


### MCP

```{r, eval = bool_execute_heavy_chunks}
models_summary$mcp.noint = NonConvex_Cv(cv.sets = month.noint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-5, 5, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 10, length = 10),
                                         my.penalty = "mcp")
```

```{r, warning = FALSE, eval = bool_execute_heavy_chunks}
models_summary$mcp = NonConvex_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-5, 5, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 10, length = 10),
                                         my.penalty = "mcp")
```


```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$mcp.noint,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "MCP no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

TwoParErrPlot(model_list = models_summary$mcp,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "MCP yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

par(mfrow = c(1,1))
```

```{r, eval = bool_execute_heavy_chunks}
# mcp.fit.all = ncvreg(X = as.matrix(my.df.year.grouped.matrix),
#                  y = my.df.year.grouped$y,
#                  gamma = models_summary$mcp$gmin,
#                  penalty = "mcp")
# 
# plot(mcp.fit.all, log.l = TRUE)
# abline(v = log(models_summary$mcp$gmin), col = "blue", lty = 2, lwd = 2)

```

```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```


### Discrete response models

In reality the counts are discrete so it seems reasonable to also try discrete response models such as Poisson, Negative Binomial and zero inflated Poisson.
For all such cases, using the counts as response an offset has to be imposed: in analogy from what has been done assuming the continuous response the offset will be the product of the NTA Population by the number of months considered (in log scale using the canonincal log link for a Poisson GLM)

### Poisson LASSO

```{r}
# Known convergence difficult, see help here. https://cran.r-project.org/web/packages/glmnet/vignettes/glmnetFamily.pdf
glmnet.control(mxitnr = 50) # increase maximum no. of IRLS iterations allowed
```

```{r, eval = bool_execute_heavy_chunks, warning=FALSE}
models_summary$lasso.poi.noint = Lasso_Offset_CV(cv.sets = month.noint.cv.k4.sets,
                                          my.lambda.vals = exp(seq(-15, 3, 0.1)) %>% sort(decreasing = TRUE),
                                          my.family = poisson())
```

```{r, eval = bool_execute_heavy_chunks, warning=FALSE}
models_summary$lasso.poi = Lasso_Offset_CV(cv.sets = month.yesint.cv.k4.sets,
                                          my.lambda.vals = exp(seq(-15, 3, 0.1)) %>% sort(decreasing = TRUE),
                                          my.family = poisson())
```

```{r}
par(mfrow = c(1,2))
PlotOneDim(x = log(models_summary$lasso.poi.noint$lambda),
           y = models_summary$lasso.poi.noint$cv.err.matr[,"cv.err"],
           se = models_summary$lasso.poi.noint$cv.err.matr[,"cv.se"],
           x.min = log(models_summary$lasso.poi.noint$lmin),
           x.1se = log(models_summary$lasso.poi.noint$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO no interaction Poisson CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

PlotOneDim(x = log(models_summary$lasso.poi$lambda),
           y = models_summary$lasso.poi$cv.err.matr$cv.err,
           se = models_summary$lasso.poi$cv.err.matr$cv.se,
           x.min = log(models_summary$lasso.poi$lmin),
           x.1se = log(models_summary$lasso.poi$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO Poisson yes interaction CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

par(mfrow = c(1,1))
```


```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```

### Poisson Elasticnet

```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$elasticnet.poi.noint = Elastic_Offset_Cv(cv.sets = month.noint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20),
                                                        my.family = poisson())
```

```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$elasticnet.poi = Elastic_Offset_Cv(cv.sets = month.yesint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20),
                                                        my.family = poisson())
```

```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$elasticnet.poi.noint,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

TwoParErrPlot(model_list = models_summary$elasticnet.poi,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet Poisson yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

par(mfrow = c(1,1))
```

```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```

### Neg bin Lasso

Using a hierarchical specification we assume $Y_i \sim P(\mu_i \lambda_i)$ and $\lambda_i \sim Ga(\tau, \tau)$ so marginally the $Y_i$ are negative binomials with variance $\mu_i(1 + \tau \mu_i)$.
In the R parameterization adopted $\theta = 1/\tau$ which becomes another tuning parameter.


```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$lasso.negbin.noint = LASSO_NegBin_Offset_Cv(cv.sets = month.noint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-3, 5, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.theta.vals = seq(1e-5, 3, length = 20))
```

```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$lasso.negbin = LASSO_NegBin_Offset_Cv(cv.sets = month.yesint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.theta.vals = seq(1e-5, 20, length = 20))
```

```{r, eval = bool_execute_heavy_chunks}
par(mfrow = c(1,2))

TwoParErrPlot(model_list = models_summary$lasso.negbin.noint,
              row_par_name = "lambda",
              col_par_name = "theta",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "tmin",
              my.main = "LASSO Negative Binomial no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "theta")

TwoParErrPlot(model_list = models_summary$lasso.negbin,
              row_par_name = "lambda",
              col_par_name = "theta",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "tmin",
              my.main = "LASSO Negative Binomial yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "theta")

par(mfrow = c(1,1))
```

```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```

### Modelli migliori












