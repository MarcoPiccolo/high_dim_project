---
title: "Arrests 2010 NTA analysis"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
```

```{r, message=FALSE, warning=FALSE}
rm(list = ls())
gc()

library(tidyverse)
library(Matrix)
library(glmnet)
library(ncvreg)
library(viridis)
library(lattice)
```

```{r}
# Condition: if TRUE execute the computational heavy chuncks, else not
# assuming the models are been loaded from a RData file before.
# IMPORTANT: in order to reproduce the result set bool_execute_heavy_chunks = TRUE

bool_execute_heavy_chunks = FALSE

# require loading models_summary list
if(!bool_execute_heavy_chunks){
  load("arrests_nta_2010_models_summary.Rdata")
} else{
  # store each model useful information:
  # validation error and beta for best lambda
  models_summary = list()
}
```

## Preprocessing
```{r}
# read the data
my.df = read.csv("../../data/core_datasets/arrests/arrests_2010_nta.csv", stringsAsFactors = T)
```

Remove date and NTA variables, convert to factor location, month and KY_CD.
```{r}
my.df$ARREST_DATE = NULL

my.df$NTA2020 = factor(my.df$NTA2020)
my.df$MONTH = factor(my.df$MONTH)

# add NA category
my.df$KY_CD = ifelse(is.na(my.df$KY_CD), "MISSING", my.df$KY_CD)
my.df$KY_CD = factor(my.df$KY_CD)
```


```{r}
str(my.df)
```

Check for NA
```{r}
apply(my.df, 2, function(col) (sum(is.na(col))))
```


A possibility is to get rid of all NAs rows, the portion of deleted rows would be relatively small (of course we're introducing some bias here).
```{r}
nrow(na.omit(my.df)) / nrow(my.df)
```
Other possibilities would be to impute values for numerical variables (using median, mean or more sofisticated methods).
For simplicity we just delete missing values rows.
```{r}
my.df = na.omit(my.df)
```


## Description
Ideally 2010 data are our training set and 2011 data are the test set.
The goal of the analysis is to identify is some covariates are correlated with the arrests rate: more specifically if the response is well explained by some non spatial covariates alone, some spatial alone or interaction between the two.

A reasonable response variable would be the count of arrests divided by the local (space zone) population, also grouping by any other covariates value.



To get an idea of the dataset used on which models are tested a
```{r}
year_grouped = my.df %>%
  dplyr::select(-"MONTH") %>% 
  group_by_all() %>% 
  summarise(count = n())

# define the arrest rate (count / population)
year_grouped$y =  year_grouped$count / year_grouped$Pop1

```

```{r}
dim(year_grouped)
colnames(year_grouped)
```
Still a huge number of observations compared to the number of variables, but what if we add interactions?

Let's look at the distribution of the counts.
```{r}
year_grouped$count %>%
  table %>% 
  plot(main = "Arrests counts grouped by all covariates observed combinations",
       xlab = "arrest count",
       ylab = "absolute frequency")
```
We can see an inflation of ones.
The ratios present a similar frequency table. Taking the logarithm of the ratio the distribution is still (a bit less) skewed.
```{r}
par(mfrow = c(1,2))

year_grouped$y %>%
  table %>% 
  plot(main = "Arrests ratio grouped by covariates",
       xlab = "number of arrests / nta population",
       ylab = "absolute frequency")

year_grouped$y %>% 
  log %>% 
  table %>% 
  plot(main = " log arrests ratio grouped by covariates",
       xlab = "log(number of arrests / nta population)",
       ylab = "absolute frequency")

par(mfrow = c(1,1))
```



```{r}
# to include zeros
# cols_to_complete = colnames(my.df)
# cols_to_complete = setdiff(cols_to_complete, "MONTH")
# 
# # Create a data frame with all combinations of the columns
# all_combinations <- expand_grid(!!!syms(cols_to_complete))
# 
# # Summarize the data and complete with zeros
# year_grouped_zeros <- my.df %>%
#   select(-MONTH) %>%
#   group_by(across(all_of(cols_to_complete))) %>%
#   summarise(count = n(), .groups = 'drop') %>%
#   ungroup() %>%
#   right_join(all_combinations, by = cols_to_complete) %>%
#   replace_na(list(count = 0))
# 
# print(year_grouped_zeros)

```


Let's count the hypothetical number of interaction terms if ones considers only interactions between spatial zones and selected arrests covariates along with the obervations /  number of parameter ratio (underestimate since there are other variables):
```{r}
var_unique_len = apply(year_grouped,
                       2,
                       function(col) length(unique(col)))

var_unique_len
```

Not including KY_CD:
```{r}
var_unique_len["NTA2020"] *
  sum(var_unique_len[c("LAW_CAT_CD", "AGE_GROUP", "PERP_SEX", "PERP_RACE")])

nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("LAW_CAT_CD", "AGE_GROUP", "PERP_SEX", "PERP_RACE")]))
```

Including KY_CD
```{r}
var_unique_len["NTA2020"] *
  sum(var_unique_len[c("KY_CD","LAW_CAT_CD", "AGE_GROUP",
                       "PERP_SEX", "PERP_RACE")])

nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("KY_CD", "LAW_CAT_CD", "AGE_GROUP",
                       "PERP_SEX", "PERP_RACE")]))
```
We decide to not employ the MONTH time variable as a covariate but use it for a model selection method.
This is because in order to get a count measure one aggregation criterion is needed, here aggregation by month is chosen.


### Variables description

Original dataset selected variables:

Census stratification variables:


## Explorative analysis

### Arrests count vs month
```{r}
my.df %>% 
  group_by_at(vars(MONTH)) %>%
  summarise(count = n()) %>% 
  plot(main = "Arrests count by month")
```


### Arrests counts vs NTA

### Arrest counts vs other covariates

## Models

### Model selection method

Given the previously described constraints, in order to be able to apply a cross validation (CV) selection method we choose to ignore the time (MONTH) factor using MONTH as index to create the CV folds as described below.
Choose k: the number of validation sets (example k = 4) each validation set is made by grouped observations of 12 / k (3) months and the months left (9) are used to fit the model. To try to compensate and average for seasonal fluctuations the validation months are chosen as spaced as possible, for example, in the case k = 4 the first validation set is (january, may, september), the second set is (february, june, october), the third is (march, july, november) and the forth is (april, august, december); in order to make each response comparable having used a different number of months a new response is defined as the arrests ratio divided by the number of months used in the grouping. Note: this is also a way to reduce the computational burden compared to using many more months combinations.

```{r}
# Groub by months indexes
GroupByMONTHSets = function(mydf, month_indexes){
  month_grouped = mydf %>%
    filter(MONTH %in% month_indexes) %>% 
    dplyr::select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n())
  
  return(month_grouped)
}
```

Define Month indexes
```{r}
# months indexes sets
# each list contains a matrix where each row contains the used indexes
month_sets_ind = list(k4 = matrix(c(1, 5, 9,
                                    2, 6, 10,
                                    3, 7, 11,
                                    4, 8, 12),
                                  byrow = T,
                                  nrow = 4),
                      
                      k6 = matrix(c(1, 7,
                                    2, 8,
                                    3, 9,
                                    4, 10,
                                    5, 11,
                                    6, 12),
                                  byrow = T,
                                  nrow = 6))
```

In order to simplify computations we remove the KY_CD variable (hoping LAW_CAT_CD will be sufficient to describe the crime type)
when using a linear model (assuming gaussian errors) we consider the response as: 
$y = \log(\text{count} / \text{population})$ where each count is the events count obtained by grouping by all other covariates and each population is specific to each NTA.
```{r, warning=FALSE}
my.df$KY_CD = NULL

my.df.year.grouped = my.df %>%
  dplyr::select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n())


# take the log to 
my.df.year.grouped$y = log(my.df.year.grouped$count) - log(my.df.year.grouped$Pop1 + 1) - log(12)
```

```{r}

FORMULA.NO.KY_CD = as.formula("count ~. -1 -Pop1 + 
                            NTA2020:LAW_CAT_CD +
                            NTA2020:AGE_GROUP +
                            NTA2020:PERP_SEX +
                            NTA2020:PERP_RACE")

FORMULA.NO.INTERACTIONS = as.formula("count ~. -1 -Pop1")
```


```{r}
my.df.year.grouped.matrix = sparse.model.matrix(FORMULA.NO.KY_CD,
                                                my.df.year.grouped)
```


```{r}
# error functions
RMSEfun = function(true_vals, pred_vals){
  sqrt(mean( (true_vals - pred_vals)^2 ))
}
```

```{r, warning=FALSE, message=FALSE}
library(glmnet)
library(dplyr)

# Function to create cross-validation sets
MakeMonthCvSets <- function(my_df, month_sets_ind, formula) {
  cv_sets <- list()
  
  for (i in 1:nrow(month_sets_ind)) {
    # Make sublists
    cv_sets[[i]] <- list(train = list(df = NA, model_matrix = NA),
                         test = list(df = NA, model_matrix = NA))
    
    # Populate the sublists
    cv_sets[[i]]$train$df <- GroupByMONTHSets(mydf = my_df,
                                              month_indexes = setdiff(1:12, month_sets_ind[i, ]))
    
    cv_sets[[i]]$train$model_matrix <- sparse.model.matrix(formula,
                                                           data = cv_sets[[i]]$train$df)
    
    # here it's the number of months considered
    cv_sets[[i]]$train$offset.constant <- length(setdiff(1:12, ncol(month_sets_ind)))
    
    
    cv_sets[[i]]$test$df <- GroupByMONTHSets(mydf = my_df,
                                             month_indexes = month_sets_ind[i, ])
    
    cv_sets[[i]]$test$model_matrix <- sparse.model.matrix(formula,
                                                          data = cv_sets[[i]]$test$df)
    
     # here it's the number of months considered
    cv_sets[[i]]$test$offset.constant <- length(ncol(month_sets_ind))
    gc()
  }
  
  return(cv_sets)
}


# NO interaction
month.noint.cv.k4.sets = MakeMonthCvSets(my_df = my.df,
                       month_sets_ind = month_sets_ind$k4,
                       formula = FORMULA.NO.INTERACTIONS)

# Yes interaction
month.yesint.cv.k4.sets = MakeMonthCvSets(my_df = my.df,
                       month_sets_ind = month_sets_ind$k4,
                       formula = FORMULA.NO.KY_CD)


```

### Note on quantitative covariates

The simplest assumption is to assume a linear (monotone) trend of the response as a function of quantitative covariates.

### LASSO

```{r}
# general purpuose lasso CV function
Lasso_CV <- function(cv_sets,
                                  my.lambda.vals) {
  
  temp_err_matr <- matrix(NA, 
                          nrow = length(my.lambda.vals),
                          ncol = length(cv_sets))
  
  for (i in 1:length(cv_sets)) {
    temp_fit <- glmnet(x = cv_sets[[i]]$train$model_matrix,
                       y = log(cv_sets[[i]]$train$df$count) - # adjust for months and population, take logarithm
                         log(cv_sets[[i]]$train$df$Pop1 + 1) -
                         log(cv_sets[[i]]$train$offset.constant),
                       alpha = 1,
                       lambda = my.lambda.vals)
    
    pred_fit <- predict(temp_fit,
                        newx = cv_sets[[i]]$test$model_matrix)
    
    temp_err_matr[, i] <- apply(pred_fit, 2,
                                function(col) RMSEfun(true_vals = cv_sets[[i]]$test$df$count,
                                                      pred_vals = exp(col +
                                                                        log(cv_sets[[i]]$test$df$Pop1 + 1)+
                                                                        log(cv_sets[[i]]$test$offset.constant))))
    
    rm(temp_fit)
    rm(pred_fit)
    gc()
  }
  
  # Compute error matrix and lambda min and 1se
  cv_err_matr <- cbind(apply(temp_err_matr, 1, mean),
                                    apply(temp_err_matr, 1, sd) / sqrt(ncol(temp_err_matr))) %>% as.data.frame()
  
  colnames(cv_err_matr) <- c("cv.err", "cv.se")
  
  lmin_index <- which.min(cv_err_matr$cv.err)
  
  l1se_index <- which.max(cv_err_matr[,"cv.err"] <= (cv_err_matr[lmin_index,"cv.err"] + cv_err_matr[lmin_index,"cv.se"]))
  
  list_summary <- list()
  list_summary[["lambda"]] <- my.lambda.vals
  
  list_summary[["lmin"]] <- my.lambda.vals[lmin_index]
  list_summary[["l1se"]] <- my.lambda.vals[l1se_index]
  list_summary[["cv.err.matr"]] = cv_err_matr
  
  rm(cv_err_matr)
  rm(temp_err_matr)
  gc()
  
  # add all to list
  
  return(list_summary)
}

```


```{r}
# general purpouse plots function
# assuming the parameter is one-dimensional

PlotOneDim = function(x, y, se,
                      x.min, x.1se,
                      xlab, ylab, main,
                      min.leg, onese.leg){
  
  plot(x, y,
     xlab = xlab,
     ylab = ylab,
     main = main,
     pch = 16,
     type = "b")

  points(x, y + se, type = "l", col = "red")
  points(x, y - se, type = "l", col = "red")

  # min
  abline(v = x.min, lty = 2, col = "blue", lwd = 2)

  # 1.se min
  abline(v = x.1se,
        lty = 2, col = "purple", lwd = 2)


  legend("topleft",
       legend = c(min.leg, onese.leg, "se"),
       col = c("blue", "purple", "red"),
       lwd = c(2, 2, 1),
       lty = c(2, 2, 1))
  
}

```


```{r}
models_summary$lasso.noint <- Lasso_CV(month.noint.cv.k4.sets,
                                        my.lambda.vals = exp(seq(-17, -6, 0.1)) %>% sort(decreasing = T))
```

```{r}
models_summary$lasso <- Lasso_CV(month.yesint.cv.k4.sets,
                                        my.lambda.vals = exp(seq(-13, -6, 0.1)) %>% sort(decreasing = T))
```

```{r}
par(mfrow = c(1,2))

PlotOneDim(x = log(models_summary$lasso.noint$lambda),
           y = models_summary$lasso.noint$cv.err.matr[,"cv.err"],
           se = models_summary$lasso.noint$cv.err.matr[,"cv.se"],
           x.min = log(models_summary$lasso.noint$lmin),
           x.1se = log(models_summary$lasso.noint$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO no interactions CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

PlotOneDim(x = log(models_summary$lasso$lambda),
           y = models_summary$lasso$cv.err.matr$cv.err,
           se = models_summary$lasso$cv.err.matr$cv.se,
           x.min = log(models_summary$lasso$lmin),
           x.1se = log(models_summary$lasso$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO yes interaction CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

par(mfrow = c(1,1))
```

### Elasticnet

```{r}
# general purpuose elasticnet CV function
Elastic_Cv <- function(cv.sets,
                       my.lambda.vals, # decreasing sorted
                       my.alpha.vals) {
  
  temp.err.array <- array(NA,
                           dim = c(length(my.lambda.vals),
                                   length(my.alpha.vals),
                                   length(cv.sets)))
  
  # cycle over cv sets
  for (k in 1:length(cv.sets)) {
    
    # cycle over alpha values
    for(j in 1:length(my.alpha.vals)){
      
    temp_fit <- glmnet(x = cv.sets[[k]]$train$model_matrix,
                       y = log(cv_sets[[k]]$train$df$count) - # adjust for months and population, take logarithm
                         log(cv_sets[[k]]$train$df$Pop1 + 1) -
                         log(cv_sets[[k]]$train$offset.constant),
                       alpha = my.alpha.vals[j],
                       lambda = my.lambda.vals)
    
    pred_fit <- predict(temp_fit,
                        newx = cv.sets[[k]]$test$model_matrix)
    
    temp.err.array[,j,k] <- apply(pred_fit, 2,
                                function(col) RMSEfun(true_vals = cv_sets[[k]]$test$df$count,
                                                      pred_vals = exp(col +
                                                                        log(cv_sets[[k]]$test$df$Pop1 + 1) +
                                                                        log(cv_sets[[k]]$test$offset.constant))))
    
    rm(temp_fit)
    rm(pred_fit)
    gc()
      
    }
    
  }
  
  # Compute error matrix
  # rows: lambda
  # cols: alpha
  cv.err.matr <- apply(temp.err.array, c(1,2), mean)
  
  # min error lambda and alpha indexes
  min_indexes = which(cv.err.matr == min(cv.err.matr), arr.ind = TRUE)
  
  # debug
  print(min_indexes)
  
  list_summary <- list()
  list_summary[["lambda"]] <- my.lambda.vals
  list_summary[["alpha"]] <- my.alpha.vals
  
  list_summary[["lmin"]] <- my.lambda.vals[min_indexes[1]]
  list_summary[["amin"]] <- my.alpha.vals[min_indexes[2]]
  list_summary[["cv.err.matr"]] = cv.err.matr
  
  rm(cv.err.matr)
  rm(temp.err.array)
  gc()
  
  # add all to list
  
  return(list_summary)
}

```

```{r}
# generic two parameters error plots
# invert the rows order because filled.contour requires all axis values in ascending order
# the first param is assumed to be lambda: it's sorted and the logarithm is taken
TwoParErrPlot = function(model_list,
                         row_par_name = "lambda",
                         col_par_name = "alpha",
                         cv_err_matr_name = "cv.err.matr",
                         row_par_min_name = "lmin",
                         col_par_min_name = "amin",
                         my.main,
                         my.xlab = "log lambda",
                         my.ylab = "alpha"){
  
  n_row = length(model_list[[row_par_name]])
  
  # sort rows in z by increasing order to match that of lambda_vals (as required by filled.contours)
filled.contour(x = log(model_list[[row_par_name]]) %>% sort(),
               y = model_list[[col_par_name]],
               z = model_list[[cv_err_matr_name]][(1:n_row) %>% sort(decreasing = TRUE),],
               main = my.main,
               xlab = my.xlab,
               ylab = my.ylab,
               color.palette = viridis::viridis,
               nlevels = 200,
               plot.axes = { 
                 axis(1)
                 axis(2)
                 points(log(model_list[[row_par_min_name]]), model_list[[col_par_min_name]],
                            col = "red", pch = 19, cex = 1.5)})

legend("topleft",
       legend = c("min err"),
       col = "red",
       text.col = "red",
       lty = 1,
       bty = "n")
}
```


```{r}
models_summary$elasticnet.noint = Elastic_Cv(cv.sets = month.noint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-20, -8, 0.1)) %>% sort(decreasing = TRUE),
                                             my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20))
```

```{r, eval = bool_execute_heavy_chunks}

models_summary$elasticnet = Elastic_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-20, -8, 0.1)) %>% sort(decreasing = TRUE),
                                             my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20))
```

```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$elasticnet.noint,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

TwoParErrPlot(model_list = models_summary$elasticnet,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

par(mfrow = c(1,1))
```



### Scad

```{r}
# general purpuose MCP or SCAD CV function
NonConvex_Cv <- function(cv.sets,
                       my.lambda.vals, # decreasing sorted
                       my.gamma.vals,
                       my.penalty = "MCP") {
  
  temp.err.array <- array(NA,
                           dim = c(length(my.lambda.vals),
                                   length(my.gamma.vals),
                                   length(cv.sets)))
  
  # cycle over cv sets
  for (k in 1:length(cv.sets)) {
    
    # cycle over alpha values
    for(j in 1:length(my.gamma.vals)){
      
    temp_fit <- ncvreg(X = as.matrix(cv.sets[[k]]$train$model_matrix),
                       y = log(cv_sets[[k]]$train$df$count) - # adjust for months and population, take logarithm
                         log(cv_sets[[k]]$train$df$Pop1 + 1) -
                         log(cv_sets[[k]]$train$offset.constant),
                       gamma = my.gamma.vals[j],
                       lambda = my.lambda.vals,
                       penalty = my.penalty)
    
    pred_fit <- predict(temp_fit,
                        X = as.matrix(cv.sets[[k]]$test$model_matrix))
    
    temp.err.array[,j,k] <- apply(pred_fit, 2,
                                function(col) RMSEfun(true_vals = cv_sets[[k]]$test$df$count,
                                                      pred_vals = exp(col +
                                                                        log(cv_sets[[k]]$test$df$Pop1 + 1) +
                                                                        log(cv_sets[[k]]$test$offset.constant))))
    
    rm(temp_fit)
    rm(pred_fit)
    gc()
      
    }
    
  }
  
  # Compute error matrix
  # rows: lambda
  # cols: alpha
  cv.err.matr <- apply(temp.err.array, c(1,2), mean)
  
  # min error lambda and alpha indexes
  min_indexes = which(cv.err.matr == min(cv.err.matr), arr.ind = TRUE)
  
  list_summary <- list()
  list_summary[["lambda"]] <- my.lambda.vals
  list_summary[["gamma"]] <- my.gamma.vals
  
  list_summary[["lmin"]] <- my.lambda.vals[min_indexes[1]]
  list_summary[["gmin"]] <- my.gamma.vals[min_indexes[2]]
  list_summary[["cv.err.matr"]] = cv.err.matr
  
  rm(cv.err.matr)
  rm(temp.err.array)
  gc()
  
  # add all to list
  
  return(list_summary)
}

```


```{r, eval = bool_execute_heavy_chunks}
models_summary$scad.noint = NonConvex_Cv(cv.sets = month.noint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-15, 1, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 20, length = 20),
                                         my.penalty = "SCAD")
```

```{r, warning = FALSE, eval = bool_execute_heavy_chunks}
models_summary$scad = NonConvex_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-15, 1, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 20, length = 20),
                                         my.penalty = "SCAD")

```

```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$scad.noint,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "SCAD no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

TwoParErrPlot(model_list = models_summary$scad,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "SCAD yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

par(mfrow = c(1,1))

```

```{r,  warning=FALSE, eval = bool_execute_heavy_chunks}
scad.fit.all = ncvreg(X = as.matrix(my.df.year.grouped.matrix),
                 y = my.df.year.grouped$y,
                 gamma = models_summary$scad$gmin,
                 penalty = "SCAD")

plot(scad.fit.all, log.l = TRUE)
abline(v = log(models_summary$scad$lmin), col = "blue", lty = 2, lwd = 2)

```


### MCP

```{r, eval = bool_execute_heavy_chunks}
models_summary$mcp.noint = NonConvex_Cv(cv.sets = month.noint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-15, 1, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(1+1e-05, 20, length = 20),
                                         my.penalty = "MCP")
```

```{r, warning = FALSE, eval = bool_execute_heavy_chunks}
models_summary$mcp = NonConvex_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-15, 1, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(1+1e-05, 20, length = 20),
                                         my.penalty = "MCP")
```


```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$mcp.noint,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "MCP no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

TwoParErrPlot(model_list = models_summary$mcp,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "MCP Cv yes interaction error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

par(mfrow = c(1,1))
```

```{r, eval = bool_execute_heavy_chunks}
mcp.fit.all = ncvreg(X = as.matrix(my.df.year.grouped.matrix),
                 y = my.df.year.grouped$y,
                 gamma = models_summary$mcp$gmin,
                 penalty = "MCP")

plot(mcp.fit.all, log.l = TRUE)
abline(v = log(models_summary$mcp$gmin), col = "blue", lty = 2, lwd = 2)

```

```{r}
# backup to save key model info
save(models_summary, file = "arrests_nta_2010_models_summary.Rdata")
```


### Discrete response models

In reality the counts are discrete so it seems reasonable to also try discrete response models such as Poisson, Negative Binomial and zero inflated Poisson.
For all such cases, using the counts as response an offset has to be imposed: in analogy from what has been done assuming the continuous response the offset will be the product of the NTA Population by the number of months considered (in log scale using the canonincal log link for a Poisson GLM)


```{r}
# general purpuose lasso CV function
#' @param my.offset_constant_train (int): take the log and add to train fold offset (same for all folds)
#' @param my.offset_constant_test (int): take the log and add to train fold offset ((same for all folds))
#' @param my.offset_var_name (char): take the log and add to train fold offset (assumed is a variable in cv sets)
Lasso_Offset_CV <- function(cv_sets,
                            my.lambda.vals,
                            my.response.name = "count",
                            my.family = poisson(),
                            my.offset_constant_train = 9, # 9 months
                            my.offset_constant_test = 3, # 12 - 9 months
                            my.offset_var_name = "Pop1") {
  
  temp_err_matr <- matrix(NA, 
                          nrow = length(my.lambda.vals),
                          ncol = length(cv_sets))
  
  for (i in 1:length(cv_sets)) {
    temp_fit <- glmnet(x = cv_sets[[i]]$train$model_matrix,
                       y = cv_sets[[i]]$train$df[[my.response.name]],
                       alpha = 1,
                       lambda = my.lambda.vals,
                       family = my.family,
                       offset = log(my.offset_constant_train) + log(cv_sets[[i]]$train$df[[my.offset_var_name]] + 1))
    
    pred_fit <- predict(temp_fit,
                        newx = cv_sets[[i]]$test$model_matrix,
                        newoffset = log(my.offset_constant_test) + log(cv_sets[[i]]$test$df[[my.offset_var_name]] + 1),
                        type = "response")
    
    temp_err_matr[, i] <- apply(pred_fit, 2,
                                function(col) RMSEfun(true_vals = cv_sets[[i]]$test$df[[my.response.name]],
                                                      pred_vals = col))
    
    rm(temp_fit)
    rm(pred_fit)
    gc()
  }
  
  # Compute error matrix and lambda min and 1se
  cv_err_matr <- cbind(apply(temp_err_matr, 1, mean),
                                    apply(temp_err_matr, 1, sd) / sqrt(ncol(temp_err_matr))) %>% as.data.frame()
  
  colnames(cv_err_matr) <- c("cv.err", "cv.se")
  
  lmin_index <- which.min(cv_err_matr$cv.err)
  
  l1se_index <- which.max(cv_err_matr[,"cv.err"] <= (cv_err_matr[lmin_index,"cv.err"] + cv_err_matr[lmin_index,"cv.se"]))
  
  list_summary <- list()
  list_summary[["lambda"]] <- my.lambda.vals
  
  list_summary[["lmin"]] <- my.lambda.vals[lmin_index]
  list_summary[["l1se"]] <- my.lambda.vals[l1se_index]
  list_summary[["cv.err.matr"]] = cv_err_matr
  
  rm(cv_err_matr)
  rm(temp_err_matr)
  gc()
  
  # add all to list
  
  return(list_summary)
}

```


### Poisson LASSO

```{r}
# Known convergence difficult, see help here. https://cran.r-project.org/web/packages/glmnet/vignettes/glmnetFamily.pdf
glmnet.control(mxitnr = 50) # increase maximum no. of IRLS iterations allowed
```

```{r, eval = bool_execute_heavy_chunks}
models_summary$lasso.poi.noint = Lasso_Offset_CV(cv_sets = month.noint.cv.k4.sets,
                                          my.lambda.vals = exp(seq(-15, 3, 0.1)) %>% sort(decreasing = TRUE),
                                          my.response.name = "count",
                                          my.family = poisson(),
                                          my.offset_constant_train = 9, # 9 months
                                          my.offset_constant_test = 3, # 12 - 9 months
                                          my.offset_var_name = "Pop1")
```

```{r, eval = bool_execute_heavy_chunks, warning=FALSE}
models_summary$lasso.poi = Lasso_Offset_CV(cv_sets = month.yesint.cv.k4.sets,
                                          my.lambda.vals = exp(seq(-15, 3, 0.1)) %>% sort(decreasing = TRUE),
                                          my.response.name = "count",
                                          my.family = poisson(),
                                          my.offset_constant_train = 9, # 9 months
                                          my.offset_constant_test = 3, # 12 - 9 months
                                          my.offset_var_name = "Pop1")
```

```{r}
par(mfrow = c(1,2))
PlotOneDim(x = log(models_summary$lasso.poi.noint$lambda),
           y = models_summary$lasso.poi.noint$cv.err.matr[,"cv.err"],
           se = models_summary$lasso.poi.noint$cv.err.matr[,"cv.se"],
           x.min = log(models_summary$lasso.poi.noint$lmin),
           x.1se = log(models_summary$lasso.poi.noint$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO no interaction Poisson CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

PlotOneDim(x = log(models_summary$lasso.poi$lambda),
           y = models_summary$lasso.poi$cv.err.matr$cv.err,
           se = models_summary$lasso.poi$cv.err.matr$cv.se,
           x.min = log(models_summary$lasso.poi$lmin),
           x.1se = log(models_summary$lasso.poi$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO Poisson yes interaction CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

par(mfrow = c(1,1))
```


```{r}
# backup to save key model info
save(models_summary, file = "arrests_nta_2010_models_summary.Rdata")
```

### Poisson Elasticnet
```{r}
# general purpuose elasticnet CV function
Elastic_Offset_Cv <- function(cv.sets,
                            my.lambda.vals,
                            my.alpha.vals,
                            my.response.name = "count",
                            my.family = poisson(),
                            my.offset_constant_train = 9, # 9 months
                            my.offset_constant_test = 3, # 12 - 9 months
                            my.offset_var_name = "Pop1") {
  
  temp.err.array <- array(NA,
                           dim = c(length(my.lambda.vals),
                                   length(my.alpha.vals),
                                   length(cv.sets)))
  
  # cycle over cv sets
  for (k in 1:length(cv.sets)) {
    
    # cycle over alpha values
    for(j in 1:length(my.alpha.vals)){
      
    temp_fit <- glmnet(x = cv.sets[[k]]$train$model_matrix,
                       y = cv.sets[[k]]$train$df$y,
                       alpha = my.alpha.vals[j],
                       lambda = my.lambda.vals,
                       family = my.family,
                       offset = log(my.offset_constant_train) + log(cv.sets[[k]]$train$df[[my.offset_var_name]]+1))
    
    pred_fit <- predict(temp_fit,
                        newx = cv.sets[[k]]$test$model_matrix,
                        newoffset = log(my.offset_constant_test) + log(cv.sets[[k]]$test$df[[my.offset_var_name]]+1))
    
    temp.err.array[,j,k] <- apply(pred_fit, 2,
                                function(col) RMSEfun(true_vals = cv.sets[[k]]$test$df[[my.response.name]],
                                                      pred_vals = col))
    
    rm(temp_fit)
    rm(pred_fit)
    gc()
      
    }
    
  }
  
  # Compute error matrix
  # rows: lambda
  # cols: alpha
  cv.err.matr <- apply(temp.err.array, c(1,2), mean)
  
  # min error lambda and alpha indexes
  min_indexes = which(cv.err.matr == min(cv.err.matr), arr.ind = TRUE)
  
  list_summary <- list()
  list_summary[["lambda"]] <- my.lambda.vals
  list_summary[["alpha"]] <- my.alpha.vals
  
  list_summary[["lmin"]] <- my.lambda.vals[min_indexes[1]]
  list_summary[["amin"]] <- my.alpha.vals[min_indexes[2]]
  list_summary[["cv.err.matr"]] = cv.err.matr
  
  rm(cv.err.matr)
  rm(temp.err.array)
  gc()
  
  # add all to list
  
  return(list_summary)
}

```


```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$elasticnet.poi.noint = Elastic_Offset_Cv(cv.sets = month.noint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20),
                                                        my.response.name = "count",
                                                        my.family = poisson(),
                                                        my.offset_constant_train = 9, # 9 months
                                                        my.offset_constant_test = 3, # 12 - 9 months
                                                        my.offset_var_name = "Pop1")
```

```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$elasticnet.poi = Elastic_Offset_Cv(cv.sets = month.yesint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20),
                                                        my.response.name = "count",
                                                        my.family = poisson(),
                                                        my.offset_constant_train = 9, # 9 months
                                                        my.offset_constant_test = 3, # 12 - 9 months
                                                        my.offset_var_name = "Pop1")
```

```{r}
par(mfrow = c(1,2))
TwoParErrPlot(model_list = models_summary$elasticnet.poi.noint,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

TwoParErrPlot(model_list = models_summary$elasticnet.poi,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet Poisson yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

par(mfrow = c(1,1))
```

```{r}
# backup to save key model info
save(models_summary, file = "arrests_nta_2010_models_summary.Rdata")
```

### Neg bin Lasso

Using a hierarchical specification we assume $Y_i \sim P(\mu_i \lambda_i)$ and $\lambda_i \sim Ga(\tau, \tau)$ so marginally the $Y_i$ are negative binomials with variance $\mu_i(1 + \tau \mu_i)$.
In the R parameterization adopted $\theta = 1/\tau$ which becomes another tuning parameter.

```{r, message=FALSE, warning=FALSE}
library(MASS)
```

```{r}
# general purpuose negbin CV function
LASSO_NegBin_Offset_Cv <- function(cv.sets,
                            my.lambda.vals,
                            my.theta.vals,
                            my.response.name = "count",
                            my.offset_constant_train = 9, # 9 months
                            my.offset_constant_test = 3, # 12 - 9 months
                            my.offset_var_name = "Pop1") {
  
  temp.err.array <- array(NA,
                           dim = c(length(my.lambda.vals),
                                   length(my.theta.vals),
                                   length(cv.sets)))
  
  # cycle over cv sets
  for (k in 1:length(cv.sets)) {
    
    # cycle over alpha values
    for(j in 1:length(my.theta.vals)){
      
    temp_fit <- glmnet(x = cv.sets[[k]]$train$model_matrix,
                       y = cv.sets[[k]]$train$df$y,
                       alpha = 1,
                       lambda = my.lambda.vals,
                       family = negative.binomial(theta = my.theta.vals[j]),
                       offset = log(my.offset_constant_train) + log(cv.sets[[k]]$train$df[[my.offset_var_name]]+1))
    
    pred_fit <- predict(temp_fit,
                        newx = cv.sets[[k]]$test$model_matrix,
                        newoffset = log(my.offset_constant_test) + log(cv.sets[[k]]$test$df[[my.offset_var_name]]+1))
    
    temp.err.array[,j,k] <- apply(pred_fit, 2,
                                function(col) RMSEfun(true_vals = cv.sets[[k]]$test$df[[my.response.name]],
                                                      pred_vals = col))
    
    rm(temp_fit)
    rm(pred_fit)
    gc()
      
    }
    
  }
  
  # Compute error matrix
  # rows: lambda
  # cols: alpha
  cv.err.matr <- apply(temp.err.array, c(1,2), mean)
  
  # min error lambda and alpha indexes
  min_indexes = which(cv.err.matr == min(cv.err.matr), arr.ind = TRUE)
  
  
  list_summary <- list()
  list_summary[["lambda"]] <- my.lambda.vals
  list_summary[["theta"]] <- my.theta.vals
  
  list_summary[["lmin"]] <- my.lambda.vals[min_indexes[1]]
  list_summary[["tmin"]] <- my.theta.vals[min_indexes[2]]
  list_summary[["cv.err.matr"]] = cv.err.matr
  
  rm(cv.err.matr)
  rm(temp.err.array)
  gc()
  
  # add all to list
  
  return(list_summary)
}

```


```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$lasso.negbin.noint = LASSO_NegBin_Offset_Cv(cv.sets = month.noint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-3, 5, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.theta.vals = seq(1e-5, 3, length = 20),
                                                        my.response.name = "count",
                                                        my.offset_constant_train = 9, # 9 months
                                                        my.offset_constant_test = 3, # 12 - 9 months
                                                        my.offset_var_name = "Pop1")
```

```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$lasso.negbin = LASSO_NegBin_Offset_Cv(cv.sets = month.yesint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.theta.vals = seq(1e-5, 20, length = 20),
                                                        my.response.name = "count",
                                                        my.offset_constant_train = 9, # 9 months
                                                        my.offset_constant_test = 3, # 12 - 9 months
                                                        my.offset_var_name = "Pop1")
```

```{r}
par(mfrow = c(1,2))

TwoParErrPlot(model_list = models_summary$lasso.negbin.noint,
              row_par_name = "lambda",
              col_par_name = "theta",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "tmin",
              my.main = "LASSO Negative Binomial no interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "theta")

TwoParErrPlot(model_list = models_summary$lasso.negbin,
              row_par_name = "lambda",
              col_par_name = "theta",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "tmin",
              my.main = "LASSO Negative Binomial yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "theta")

par(mfrow = c(1,1))
```

```{r}
# backup to save key model info
save(models_summary, file = "arrests_nta_2010_models_summary.Rdata")
```












