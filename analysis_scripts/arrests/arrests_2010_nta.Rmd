---
title: "Arrests 2010 NTA analysis"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
```

```{r, message=FALSE, warning=FALSE}
rm(list = ls())
gc()

library(tidyverse)
library(Matrix)
```

```{r}
# Condition: if TRUE execute the computational heavy chuncks, else not
# assuming the models are been loaded from a RData file before.
# IMPORTANT: in order to reproduce the result set bool_execute_heavy_chunks = TRUE

bool_execute_heavy_chunks = FALSE

# require loading models_summary list
if(!bool_execute_heavy_chunks){
  load("arrests_nta_2010_models_summary.Rdata")
}

else{
  # store each model useful information:
  # validation error and beta for best lambda
  models_summary = list()
}
```

## Preprocessing
```{r}
# read the data
my.df = read.csv("../../data/core_datasets/arrests/arrests_2010_nta.csv", stringsAsFactors = T)
```

Remove date and NTA variables, convert to factor location, month and KY_CD.
```{r}
my.df$ARREST_DATE = NULL

my.df$NTA2020 = factor(my.df$NTA2020)
my.df$MONTH = factor(my.df$MONTH)

# add NA category
my.df$KY_CD = ifelse(is.na(my.df$KY_CD), "MISSING", my.df$KY_CD)
my.df$KY_CD = factor(my.df$KY_CD)
```


```{r}
str(my.df)
```

Check for NA
```{r}
apply(my.df, 2, function(col) (sum(is.na(col))))
```


A possibility is to get rid of all NAs rows, the portion of deleted rows would be relatively small (of course we're introducing some bias here).
```{r}
nrow(na.omit(my.df)) / nrow(my.df)
```
Other possibilities would be to impute values for numerical variables (using median, mean or more sofisticated methods).
For simplicity we just delete missing values rows.
```{r}
my.df = na.omit(my.df)
```


## Description
Ideally 2010 data are our training set and 2011 data are the test set.
The goal of the analysis is to identify is some covariates are correlated with the arrests rate: more specifically if the response is well explained by some non spatial covariates alone, some spatial alone or interaction between the two.

A reasonable response variable would be the count of arrests divided by the local (space zone) population, also grouping by any other covariates value.



To get an idea of the dataset used on which models are tested a
```{r}
year_grouped = my.df %>%
  select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n())

# define the arrest rate (count / population)
year_grouped$y =  year_grouped$count / year_grouped$Pop1

```

```{r}
dim(year_grouped)
colnames(year_grouped)
```
Still a huge number of observations compared to the number of variables, but what if we add interactions?

Let's look at the distribution of the counts.
```{r}
year_grouped$count %>%
  table %>% 
  plot(main = "Arrests counts grouped by all covariates observed combinations",
       xlab = "arrest count",
       ylab = "absolute frequency")
```
We can see an inflation of ones.
The ratios present a similar frequency table. Taking the logarithm of the ratio the distribution is still (a bit less) skewed.
```{r}
par(mfrow = c(1,2))

year_grouped$y %>%
  table %>% 
  plot(main = "Arrests ratio grouped by covariates",
       xlab = "number of arrests / nta population",
       ylab = "absolute frequency")

year_grouped$y %>% 
  log %>% 
  table %>% 
  plot(main = " log arrests ratio grouped by covariates",
       xlab = "log(number of arrests / nta population)",
       ylab = "absolute frequency")

par(mfrow = c(1,1))
```



```{r}
# to include zeros
# cols_to_complete = colnames(my.df)
# cols_to_complete = setdiff(cols_to_complete, "MONTH")
# 
# # Create a data frame with all combinations of the columns
# all_combinations <- expand_grid(!!!syms(cols_to_complete))
# 
# # Summarize the data and complete with zeros
# year_grouped_zeros <- my.df %>%
#   select(-MONTH) %>%
#   group_by(across(all_of(cols_to_complete))) %>%
#   summarise(count = n(), .groups = 'drop') %>%
#   ungroup() %>%
#   right_join(all_combinations, by = cols_to_complete) %>%
#   replace_na(list(count = 0))
# 
# print(year_grouped_zeros)

```


Let's count the hypothetical number of interaction terms if ones considers only interactions between spatial zones and selected arrests covariates along with the obervations /  number of parameter ratio (underestimate since there are other variables):
```{r}
var_unique_len = apply(year_grouped,
                       2,
                       function(col) length(unique(col)))

var_unique_len
```

Not including KY_CD:
```{r}
var_unique_len["NTA2020"] *
  sum(var_unique_len[c("LAW_CAT_CD", "AGE_GROUP", "PERP_SEX", "PERP_RACE")])

nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("LAW_CAT_CD", "AGE_GROUP", "PERP_SEX", "PERP_RACE")]))
```

Including KY_CD
```{r}
var_unique_len["NTA2020"] *
  sum(var_unique_len[c("KY_CD","LAW_CAT_CD", "AGE_GROUP",
                       "PERP_SEX", "PERP_RACE")])

nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("KY_CD", "LAW_CAT_CD", "AGE_GROUP",
                       "PERP_SEX", "PERP_RACE")]))
```
We decide to not employ the MONTH time variable as a covariate but use it for a model selection method.


### Variables description

Original dataset selected variables:

Census stratification variables:


## Explorative analysis

### Arrests count vs month
```{r}
my.df %>% 
  group_by_at(vars(MONTH)) %>%
  summarise(count = n()) %>% 
  plot(main = "Arrests count by month")
```


### Arrests counts vs NTA

### Arrest counts vs other covariates

## Models

### Model selection method

Given the previously described constraints, in order to be able to apply a cross validation (CV) selection method we choose to ignore the time (MONTH) factor using MONTH as index to create the CV folds as described below.
Choose k: the number of validation sets (example k = 4) each validation set is made by grouped observations of 12 / k (3) months and the months left are used to fit the model. To try to compensate and average for seasonal fluctuations the validation months are chosen as spaced as possible, for example, in the case k = 4 the first validation set is (january, may, september), the second set is (february, june, october), the third is (march, july, november) and the forth is (april, august, december); in order to make each response comparable having used a different number of months a new response is defined as the arrests ratio divided by the number of months used in the grouping.

```{r}
# Groub by months indexes
GroupByMONTHSets = function(mydf, month_indexes){
  month_grouped = mydf %>%
    filter(MONTH %in% month_indexes) %>% 
    select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n())
  
  month_grouped$y = month_grouped$count /
    (month_grouped$Pop1 * length(month_indexes))
  
  return(month_grouped)
}
```

Define Month indexes
```{r}
# months indexes sets
# each list contains a matrix where each row contains the used indexes
month_sets_ind = list(k4 = matrix(c(1, 5, 9,
                                    2, 6, 10,
                                    3, 7, 11,
                                    4, 8, 12),
                                  byrow = T,
                                  nrow = 4),
                      k6 = matrix(c(1, 7,
                                    2, 8,
                                    3, 9,
                                    4, 10,
                                    5, 11,
                                    6, 12),
                                  byrow = T,
                                  nrow = 6))
```

In order to simplify computations we remove the KY_CD variable (hoping LAW_CAT_CD will be sufficient to describe the crime type)
when using a linear model (assuming gaussian errors) we consider the response as: 
$y = \log(\text{count} / \text{population})$ where each count is the events count obtained by grouping by all other covariates and each population is specific to each NTA.
```{r, warning=FALSE}
my.df$KY_CD = NULL

my.df.year.grouped = my.df %>%
  select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n())


# take the log to 
my.df.year.grouped$y = log(my.df.year.grouped$count / my.df.year.grouped$Pop1)
```


```{r}
my.df.year.grouped.matrix = sparse.model.matrix(FORMULA.NO.KY_CD.matrix,
                                                my.df.year.grouped)
```


```{r}

FORMULA.NO.KY_CD.matrix = as.formula("y ~. -1 - count - Pop1 + 
                            NTA2020:LAW_CAT_CD +
                            NTA2020:AGE_GROUP +
                            NTA2020:PERP_SEX +
                            NTA2020:PERP_RACE")

FORMULA.NO.KY_CD = as.formula("y ~. -1 -Pop1 - count + 
                            NTA2020:LAW_CAT_CD +
                            NTA2020:AGE_GROUP +
                            NTA2020:PERP_SEX +
                            NTA2020:PERP_RACE")
```

Error functions
```{r}
RMSEfun = function(true_vals, pred_vals){
  sqrt(mean( (true_vals - pred_vals)^2 ))
}
```



Model matrices: omit KY_CD variables due to computational issues.
Make a list of fit and validation sets:

```{r}
# function to write

# MakeCVSets = function(original_df, )

```


```{r, warning=FALSE, message=FALSE}

library(dplyr)
library(Matrix)

month.fit.val.k4.sets = list()

for (i in 1:nrow(month_sets_ind$k4)){
  # make sublists
  month.fit.val.k4.sets[[i]] = list(train = list(y = NA,model_matrix = NA),
                                    test = list(y = NA,model_matrix = NA))
  
  # populate the sublists
  # keep also the all df
  month.fit.val.k4.sets[[i]]$train$df = GroupByMONTHSets(mydf = my.df,
                       month_indexes = setdiff(1:12, month_sets_ind$k4[i,]))
  
  month.fit.val.k4.sets[[i]]$train$model_matrix = sparse.model.matrix(FORMULA.NO.KY_CD.matrix,
                          data = month.fit.val.k4.sets[[i]]$train$df)
  
  
  month.fit.val.k4.sets[[i]]$test$df = GroupByMONTHSets(mydf = my.df,
                       month_indexes = month_sets_ind$k4[i,])
  
  month.fit.val.k4.sets[[i]]$test$model_matrix = sparse.model.matrix(FORMULA.NO.KY_CD.matrix,
                          data = month.fit.val.k4.sets[[i]]$test$df)
  gc()
}

```

### Note on quantitative covariates

The simplest assumption is to assume a linear (monotone) trend of the response as a function of quantitative covariates.

### LASSO

```{r}
library(glmnet)



# first compute a lambda grid on all grouped dataset

lasso.fit.all = glmnet(x = my.df.year.grouped.matrix,
                 y = my.df.year.grouped$y,
                 alpha = 1)

models_summary$lasso = list()
models_summary$lasso$lambda_vals = exp(seq(-13, -6, 0.1)) %>% sort(decreasing = T)

temp.err.matr = matrix(NA, 
                      nrow = length(models_summary$lasso$lambda_vals),
                      ncol = nrow(month_sets_ind$k4))


for (i in 1:dim(temp.err.matr)[2]){
  temp.fit = glmnet(x = month.fit.val.k4.sets[[i]]$train$model_matrix,
                 y = month.fit.val.k4.sets[[i]]$train$df$y,
                 alpha = 1,
                 lambda = models_summary$lasso$lambda_vals)
  
  pred.fit = predict.glmnet(temp.fit,
                              newx = month.fit.val.k4.sets[[i]]$test$model_matrix)
  
  temp.err.matr[,i] = apply(pred.fit, 2,
                                  function(col) RMSEfun(true_vals = exp(month.fit.val.k4.sets[[i]]$test$df$y),
                                                        pred_vals = exp(col)))
  
  rm(temp.fit)
  rm(pred.fit)
  gc()
  
}

# compute error matrix and lambda min and 1se
models_summary$lasso$cv.err.matr = cbind(models_summary$lasso$lambda_vals,
                          apply(temp.err.matr,1,mean),
                          apply(temp.err.matr,1,sd)/ncol(temp.err.matr)) %>% as.data.frame()

colnames(models_summary$lasso$cv.err.matr) = c("lambda", "cv.err", "cv.se")

models_summary$lasso$lmin.index = which.min(models_summary$lasso$cv.err.matr$cv.err)

models_summary$lasso$l1se.index = which.max(models_summary$lasso$cv.err.matr$cv.err <=
                                              (models_summary$lasso$cv.err.matr$cv.err[models_summary$lasso$lmin.index] +
                                              models_summary$lasso$cv.err.matr$cv.se[models_summary$lasso$lmin.index]))

models_summary$lasso$lmin = models_summary$lasso$cv.err.matr$lambda[models_summary$lasso$lmin.index]
models_summary$lasso$l1se = models_summary$lasso$cv.err.matr$lambda[models_summary$lasso$l1se.index]
```


```{r}
# general purpouse plots function
# assuming the parameter is one-dimensional

PlotOneDim = function(x, y, se,
                      x.min, x.1se,
                      xlab, ylab, main,
                      min.leg, onese.leg){
  
  plot(x, y,
     xlab = xlab,
     ylab = ylab,
     main = main,
     pch = 16,
     type = "b")

  points(x, y + se, type = "l", col = "red")
  points(x, y - se, type = "l", col = "red")

  # min
  abline(v = x.min, lty = 2, col = "blue", lwd = 2)

  # 1.se min
  abline(v = x.1se,
        lty = 2, col = "purple", lwd = 2)


  legend("topleft",
       legend = c(min.leg, onese.leg, "se"),
       col = c("blue", "purple", "red"),
       lwd = c(2, 2, 1),
       lty = c(2, 2, 1))
  
}

```



```{r}

PlotOneDim(x = log(models_summary$lasso$cv.err.matr$lambda),
           y = models_summary$lasso$cv.err.matr$cv.err,
           se = models_summary$lasso$cv.err.matr$cv.se,
           x.min = log(models_summary$lasso$lmin),
           x.1se = log(models_summary$lasso$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")
```
Best $lambda$ is close to zero, so the best solution seems OLS classical solution.

Check coefficients paths

```{r}
plot(lasso.fit.all, xvar = "lambda", main = "LASSO beta path")
abline(v = log(models_summary$lasso$lmin), col = "blue", lty = 2, lwd = 2)
abline(v = log(models_summary$lasso$l1se), col = "purple", lty = 2, lwd = 2)
```



### Elasticnet

Selecting a grid of 20 $\alpha \in (0,1)$.
```{r, eval = bool_execute_heavy_chunks}
models_summary$elasticnet = list()

models_summary$elasticnet$lambda_vals =  exp(seq(-13, -8, 0.1)) # glmnet sorts by decreasing order
models_summary$elasticnet$alpha_vals = seq(1e-5, 1 - 1e-5, length = 20)

# make cv error matrix
temp.err.array = array(NA,
                       dim = c(length(models_summary$elasticnet$lambda_vals),
                               length(models_summary$elasticnet$alpha_vals),
                               nrow(month_sets_ind$k4)))

# cycle over cv sets
for (k in 1:dim(temp.err.array)[3]){
  
  # iterate over alpha
for(j in 1:dim(temp.err.array)[2]){

  temp.fit = glmnet(x = month.fit.val.k4.sets[[k]]$train$model_matrix,
                 y = month.fit.val.k4.sets[[k]]$train$df$y,
                 alpha = models_summary$elasticnet$alpha_vals[j],
                 lambda = models_summary$elasticnet$lambda_vals) # all lambda path
  
  pred.fit = predict.glmnet(temp.fit,
                              newx = month.fit.val.k4.sets[[k]]$test$model_matrix)
  
  
  temp.err.array[,j,k] = apply(pred.fit, 2,
                                  function(col) RMSEfun(true_vals = exp(month.fit.val.k4.sets[[k]]$test$df$y),
                                                        pred_vals = exp(col)))
  
  rm(temp.fit)
  rm(pred.fit)
  gc()
  
  }
  
}

# average error over all cv folds
models_summary$elasticnet$cv.err.matrix = apply(temp.err.array, c(1,2), mean)

# due to plotting reasons: invert the rows order
# so that the lambdas are in increasing order
models_summary$elasticnet$cv.err.matrix = models_summary$elasticnet$cv.err.matrix[rev(seq_len(nrow(models_summary$elasticnet$cv.err.matrix))), ]

models_summary$elasticnet$cv.err.min = min(models_summary$elasticnet$cv.err.matrix)

models_summary$elasticnet$err.min.indexes.lambda.alpha = which(models_summary$elasticnet$cv.err.matrix ==
        models_summary$elasticnet$cv.err.min,
      arr.ind = TRUE)

models_summary$elasticnet$lmin = models_summary$elasticnet$lambda_vals[models_summary$elasticnet$err.min.indexes.lambda.alpha[1]]

models_summary$elasticnet$amin = models_summary$elasticnet$alpha_vals[models_summary$elasticnet$err.min.indexes.lambda.alpha[2]]

```

```{r}
filled.contour(x = log(models_summary$elasticnet$lambda_vals),
               y = models_summary$elasticnet$alpha_vals,
               z = models_summary$elasticnet$cv.err.matrix,
               main = "Elasticnet Cv error contour",
               xlab = "log lambda",
               ylab = "alpha",
               color.palette = viridis::viridis,
               nlevels = 200)

points(log(models_summary$elasticnet$lmin),
      models_summary$elasticnet$amin,
      col = "white", pch = 16)

abline(v = log(models_summary$elasticnet$lmin), col = "white", lty = 2)
abline(h = models_summary$elasticnet$amin, col = "white", lty = 2)

legend("topleft",
       legend = c("min err"),
       col = "white",
       text.col = "white",
       lty = 2,
       bty = "n")
```

The best selected model is close to a ridge regression since $\alpha$ is close to zero.


### Scad

```{r, warning = FALSE, eval = bool_execute_heavy_chunks}
library(ncvreg)

models_summary$scad = list()

models_summary$scad$lambda_vals =  exp(seq(-15, 1, 0.2)) %>% sort(decreasing = TRUE)
models_summary$scad$gamma_vals = seq(2+1e-05, 20, length = 20)

# make cv error matrix
temp.err.array = array(NA,
                       dim = c(length(models_summary$scad$lambda_vals),
                               length(models_summary$scad$gamma_vals),
                               nrow(month_sets_ind$k4)))

# cycle over cv sets
for (k in 1:dim(temp.err.array)[3]){
  
  # iterate over alpha
for(j in 1:dim(temp.err.array)[2]){
  
  # ncvreg doens't accept sparse matrices
  temp.fit = ncvreg(X = as.matrix(month.fit.val.k4.sets[[k]]$train$model_matrix),
                 y = month.fit.val.k4.sets[[k]]$train$df$y,
                 gamma = models_summary$scad$gamma_vals[j],
                 lambda = models_summary$scad$lambda_vals %>% sort(decreasing = T),
                 penalty = "SCAD") # all lambda path
  
  pred.fit = predict(temp.fit,
                     X = as.matrix(month.fit.val.k4.sets[[k]]$test$model_matrix))
  
  
  temp.err.array[,j,k] = apply(pred.fit, 2,
                                  function(col) RMSEfun(true_vals = exp(month.fit.val.k4.sets[[k]]$test$df$y),
                                                        pred_vals = exp(col)))
  
  rm(temp.fit)
  rm(pred.fit)
  gc()
  
  }
  
}

# average error over all cv folds
models_summary$scad$cv.err.matrix = apply(temp.err.array, c(1,2), mean)

# due to plotting reasons: invert the rows order
# so that the lambdas are in increasing order
models_summary$scad$cv.err.matrix = models_summary$scad$cv.err.matrix[rev(seq_len(nrow(models_summary$scad$cv.err.matrix))), ]

models_summary$scad$cv.err.min = min(models_summary$scad$cv.err.matrix)

models_summary$scad$err.min.indexes.lambda.gamma = which(models_summary$scad$cv.err.matrix ==
        models_summary$scad$cv.err.min,
      arr.ind = TRUE)

models_summary$scad$lmin = models_summary$scad$lambda_vals[models_summary$scad$err.min.indexes.lambda.gamma[1]]

models_summary$scad$gmin = models_summary$scad$gamma_vals[models_summary$scad$err.min.indexes.lambda.gamma[2]]

```

```{r}
filled.contour(x = log(models_summary$scad$lambda_vals),
               y = models_summary$scad$gamma_vals,
               z = models_summary$scad$cv.err.matrix,
               main = "SCAD Cv error contour",
               xlab = "log lambda",
               ylab = "gamma",
               color.palette = viridis::viridis,
               nlevels = 200)

points(log(models_summary$scad$lmin),
      models_summary$scad$gmin,
      col = "white", pch = 16)

abline(v = log(models_summary$scad$lmin), col = "white", lty = 2)
abline(h = models_summary$scad$gmin, col = "white", lty = 2)

legend("topleft",
       legend = c("min err"),
       col = "white",
       text.col = "white",
       lty = 2,
       bty = "n")
```
Check if the estimate is in the non convex region.

```{r,  warning=FALSE}
scad.fit.all = ncvreg(X = as.matrix(my.df.year.grouped.matrix),
                 y = my.df.year.grouped$y,
                 gamma = models_summary$scad$gmin,
                 lambda = models_summary$scad$lambda_vals %>% sort(decreasing = T),
                 penalty = "SCAD")

plot(scad.fit.all, log.l = TRUE)
abline(v = log(models_summary$scad$lmin), col = "blue", lty = 2, lwd = 2)

```


### MCP

```{r, warning = FALSE, eval = bool_execute_heavy_chunks}
library(ncvreg)

models_summary$mcp = list()

models_summary$mcp$lambda_vals =  exp(seq(-20, 5, 0.5)) %>% sort(decreasing = TRUE)
models_summary$mcp$gamma_vals = seq(1+1e-05, 20, length = 20)

# make cv error matrix
temp.err.array = array(NA,
                       dim = c(length(models_summary$mcp$lambda_vals),
                               length(models_summary$mcp$gamma_vals),
                               nrow(month_sets_ind$k4)))

# cycle over cv sets
for (k in 1:dim(temp.err.array)[3]){
  
  # iterate over alpha
for(j in 1:dim(temp.err.array)[2]){
  
  # ncvreg doens't accept sparse matrices
  temp.fit = ncvreg(X = as.matrix(month.fit.val.k4.sets[[k]]$train$model_matrix),
                 y = month.fit.val.k4.sets[[k]]$train$df$y,
                 gamma = models_summary$mcp$gamma_vals[j],
                 lambda = models_summary$mcp$lambda_vals,
                 penalty = "MCP") # all lambda path
  
  pred.fit = predict(temp.fit,
                     X = as.matrix(month.fit.val.k4.sets[[k]]$test$model_matrix))
  
  
  temp.err.array[,j,k] = apply(pred.fit, 2,
                                  function(col) RMSEfun(true_vals = exp(month.fit.val.k4.sets[[k]]$test$df$y),
                                                        pred_vals = exp(col)))
  
  rm(temp.fit)
  rm(pred.fit)
  gc()
  
  }
  
}

# average error over all cv folds
models_summary$mcp$cv.err.matrix = apply(temp.err.array, c(1,2), mean)

# due to plotting reasons: invert the rows order
# so that the lambdas are in increasing order
models_summary$mcp$cv.err.matrix = models_summary$mcp$cv.err.matrix[rev(seq_len(nrow(models_summary$mcp$cv.err.matrix))), ]

models_summary$mcp$cv.err.min = min(models_summary$mcp$cv.err.matrix)

models_summary$mcp$err.min.indexes.lambda.gamma = which(models_summary$mcp$cv.err.matrix ==
        models_summary$mcp$cv.err.min,
      arr.ind = TRUE)

models_summary$mcp$lmin = models_summary$mcp$lambda_vals[models_summary$mcp$err.min.indexes.lambda.gamma[1]]

models_summary$mcp$gmin = models_summary$mcp$gamma_vals[models_summary$mcp$err.min.indexes.lambda.gamma[2]]

```

```{r}
filled.contour(x = log(models_summary$mcp$lambda_vals),
               y = models_summary$mcp$gamma_vals,
               z = models_summary$mcp$cv.err.matrix,
               main = "MCP Cv error contour",
               xlab = "log lambda",
               ylab = "gamma",
               color.palette = viridis::viridis,
               nlevels = 200)

points(log(models_summary$mcp$lmin),
      models_summary$mcp$gmin,
      col = "white", pch = 16)

abline(v = log(models_summary$mcp$lmin), col = "white", lty = 2)
abline(h = models_summary$mcp$gmin, col = "white", lty = 2)

legend("topleft",
       legend = c("min err"),
       col = "white",
       text.col = "white",
       lty = 2,
       bty = "n")
```
Check if the estimate is in the non convex region.

```{r}
mcp.fit.all = ncvreg(X = as.matrix(my.df.year.grouped.matrix),
                 y = my.df.year.grouped$y,
                 gamma = models_summary$mcp$gmin,
                 lambda = models_summary$mcp$lambda_vals %>% sort(decreasing = TRUE),
                 penalty = "MCP")

plot(mcp.fit.all)
abline(v = log(models_summary$mcp$gmin), col = "blue", lty = 2, lwd = 2)

```

```{r}
# backup to save key model info
save(models_summary, file = "arrests_nta_2010_models_summary.Rdata")
```


### Discrete response models

In reality the counts are discrete so it seems reasonable to also try discrete response models such as Poisson, Negative Binomial and zero inflated Poisson.
For all such cases, using the counts as response an offset has to be imposed: in analogy from what has been done assuming the continuous response the offset will be the product of the NTA Population by the number of months considered (in log scale using the canonincal log link for a Poisson GLM)

### Poisson LASSO

```{r}
# Known convergence difficult, see help here. https://cran.r-project.org/web/packages/glmnet/vignettes/glmnetFamily.pdf
```


```{r, eval = bool_execute_heavy_chunks}
# WARNING: to fix model matrix
glmnet.control(mxitnr = 50) # increase maximum no. of IRLS iterations allowed
# all.fit.poi = glmnet(x = sparse.model.matrix(FORMULA.POI.NO.KY_CD,
#                                          data = my.df.year.grouped),
#                  y = my.df.year.grouped$count,
#                  family = poisson(),
#                  offset = log(my.df.year.grouped$Pop1 * 12), # 12 months
#                  alpha = 1,
#                  intercept = TRUE,
#                  standardize = TRUE)

lambda.vals.poi = exp(seq(-10, 5, 0.1))

# validation error matrix
# each col
lasso.cv.error.matr = matrix(NA,
                             nrow = length(lambda.vals.poi),
                             ncol = nrow(month_sets_ind$k4))

n_months_train = ncol(month_sets_ind$k4)
n_months_test = 12 - n_months_train

for (i in 1:dim(lasso.cv.error.matr)[2]){
  temp.lasso = glmnet(x = month.fit.val.k4.sets[[i]]$train$model_matrix,
                 y = month.fit.val.k4.sets[[i]]$train$df$count,
                 family = poisson(),
                 offset = log(month.fit.val.k4.sets[[i]]$train$df$Pop1) + 
                                log(n_months_train),
                 alpha = 1,
                 lambda = lambda.vals.poi,
                 intercept = TRUE,
                 standardize = TRUE)
  
  pred.lasso = predict.glmnet(temp.lasso,
                              newx = month.fit.val.k4.sets[[i]]$test$model_matrix,
                              newoffset = log(month.fit.val.k4.sets[[i]]$test$df$Pop1) + 
                                log(n_months_test),
                              type = "response") # predict the mean
  
  # to check offset
  lasso.cv.error.matr[,i] = apply(pred.lasso,
                                  2,
                                  function(col) RMSEfun(true_vals =
                                                        month.fit.val.k4.sets[[i]]$test$df$count,
                                                        pred_vals = col))
  
  rm(temp.lasso)
  rm(pred.lasso)
  gc()
  
}
```

```{r}
# find cv error average and s.e
lasso.lambda.errs = cbind(lambda.vals.poi %>% sort(decreasing = T),
                          apply(lasso.cv.error.matr,1,mean),
                          apply(lasso.cv.error.matr,1,sd)/ncol(lasso.cv.error.matr))

```

```{r}
plot(log(lasso.lambda.errs[,1]), lasso.lambda.errs[,2],
     xlab = "log lambda",
     ylab = "RMSE",
     main = "LASSO POI CV error",
     pch = 16,
     type = "b")

points(log(lasso.lambda.errs[,1]),
       lasso.lambda.errs[,2] + lasso.lambda.errs[,3], type = "l", col = "red")
points(log(lasso.lambda.errs[,1]), lasso.lambda.errs[,2] - lasso.lambda.errs[,3], type = "l", col = "red")

lasso.min.index = which.min(lasso.lambda.errs[,2])
lasso.lmin = lasso.lambda.errs[lasso.min.index,1]

lasso.1se.index = which.max(lasso.lambda.errs[,2] <= (lasso.lambda.errs[lasso.min.index,2] +
                                                     lasso.lambda.errs[lasso.min.index,3]))
lasso.l1se = lasso.lambda.errs[lasso.1se.index,1]

# min
abline(v = log(lasso.lmin), lty = 2, col = "blue", lwd = 2)

# 1.se min
abline(v = log(lasso.l1se),
       lty = 2, col = "purple", lwd = 2)


legend("topleft",
       legend = c("lambda min", "lambda 1se", "se"),
       col = c("blue", "purple", "red"),
       lwd = c(2, 2, 1),
       lty = c(2, 2, 1))
```
There's a problem here since for smaller $\lambda$ values the algorithm does not converge.

### Poisson Elasticnet
```{r}
month.fit.val.k4.sets[[i]]$test$count %>% length()
```


### Neg bin Lasso

Using a hierarchical specification we assume $Y_i \sim P(\mu_i \lambda_i)$ and $\lambda_i \sim Ga(\tau, \tau)$ so marginally the $Y_i$ are negative binomials with variance $\mu_i(1 + \tau \mu_i)$.
In the R parameterization adopted $\theta = 1/\tau$ which becomes another tuning parameter.

```{r, message=FALSE, warning=FALSE}
library(MASS)
```

```{r, eval = bool_execute_heavy_chunks}
# WARNING: to fix model matrix
glmnet.control(mxitnr = 50) # increase maximum no. of IRLS iterations allowed
# all.fit.quasi.poi = glmnet(x = sparse.model.matrix(FORMULA.POI.NO.KY_CD,
#                                          data = my.df.year.grouped),
#                  y = my.df.year.grouped$count,
#                  family = negative.binomial(theta = 5), # another reg param.
#                  offset = log(my.df.year.grouped$Pop1 * 12), # 12 months
#                  alpha = 1,
#                  intercept = TRUE,
#                  standardize = TRUE)

lambda.vals.poi = all.fit.poi$lambda[1:35] # convergence issues for 36th lambda

# validation error matrix
# each col
lasso.cv.error.matr = matrix(NA,
                             nrow = length(lambda.vals.poi),
                             ncol = nrow(month_sets_ind$k4))

n_months_train = ncol(month_sets_ind$k4)
n_months_test = 12 - n_months_train

for (i in 1:dim(lasso.cv.error.matr)[2]){
  temp.lasso = glmnet(x = month.fit.val.k4.sets[[i]]$train$model_matrix,
                 y = month.fit.val.k4.sets[[i]]$train$count,
                 family = negative.binomial(theta = 5),
                 offset = log(month.fit.val.k4.sets[[i]]$train$Pop1) + 
                                log(n_months_train),
                 alpha = 1,
                 lambda = lambda.vals.poi,
                 intercept = TRUE,
                 standardize = TRUE)
  
  pred.lasso = predict.glmnet(temp.lasso,
                              newx = month.fit.val.k4.sets[[i]]$test$model_matrix,
                              newoffset = log(month.fit.val.k4.sets[[i]]$test$Pop1) + 
                                log(n_months_test),
                              type = "response") #predict the mean -> check
  
  # to check offset
  lasso.cv.error.matr[,i] = apply(pred.lasso,
                                  2,
                                  function(col) RMSEfun(true_vals =
                                                        month.fit.val.k4.sets[[i]]$test$count,
                                                        pred_vals = col))
  
  rm(temp.lasso)
  rm(pred.lasso)
  gc()
  
}
```


```{r}
plot(log(lasso.lambda.errs[,1]), lasso.lambda.errs[,2],
     xlab = "log lambda",
     ylab = "RMSE",
     main = "LASSO Neg Bin CV error",
     pch = 16,
     type = "b")

points(log(lasso.lambda.errs[,1]),
       lasso.lambda.errs[,2] + lasso.lambda.errs[,3], type = "l", col = "red")
points(log(lasso.lambda.errs[,1]), lasso.lambda.errs[,2] - lasso.lambda.errs[,3], type = "l", col = "red")

lasso.min.index = which.min(lasso.lambda.errs[,2])
lasso.lmin = lasso.lambda.errs[lasso.min.index,1]

lasso.1se.index = which.max(lasso.lambda.errs[,2] <= (lasso.lambda.errs[lasso.min.index,2] +
                                                     lasso.lambda.errs[lasso.min.index,3]))
lasso.l1se = lasso.lambda.errs[lasso.1se.index,1]

# min
abline(v = log(lasso.lmin), lty = 2, col = "blue", lwd = 2)

# 1.se min
abline(v = log(lasso.l1se),
       lty = 2, col = "purple", lwd = 2)


legend("topleft",
       legend = c("lambda min", "lambda 1se", "se"),
       col = c("blue", "purple", "red"),
       lwd = c(2, 2, 1),
       lty = c(2, 2, 1))
```












