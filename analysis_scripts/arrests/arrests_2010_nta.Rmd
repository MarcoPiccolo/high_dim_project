---
title: "Arrests 2010 NTA: Analisi"
output: pdf_document
eometry: margin=1cm
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
```

```{r, message=FALSE, warning=FALSE, results='hide'}
rm(list = ls())
gc(verbose = FALSE)
library(gridExtra)

# plotting parameters 1d plot
OUT.WIDTH.1d = "100%"
OUT.HEIGHT.1d = "100%"

# plotting parameters 2d plots
OUT.WIDTH = "45%"
OUT.HEIGHT = "45%"

source("functions_script.R")
```

```{r}
# Condition: if TRUE execute the computational heavy chuncks, else not
# assuming the models are been loaded from a RData file before.
# IMPORTANT: in order to reproduce the result set bool_execute_heavy_chunks = TRUE

bool_execute_heavy_chunks = FALSE
file_save_name = "arrests_nta_2010_models_summary.Rdata"

# require loading models_summary list
if(!bool_execute_heavy_chunks){
  load(file_save_name)
} else{
  # store each model useful information:
  # validation error and beta for best lambda
  models_summary = list()
}


# best models fitted on all data
estimated_best_models = TRUE
models_fitted_file_name = "models_fitted_nta_2010.Rdata"

# require loading models_summary list
if(!estimated_best_models){
  load(models_fitted_file_name)
} else{
  # store each model useful information:
  # validation error and beta for best lambda
  models_fitted_file_name = list()
}
```


```{r}
# Preprocessing -------------------------------------
# read the data
df_2010 = read.csv("../../data/final_datasets/arrests_2010_nta.csv", stringsAsFactors = T)

# test set
df_2011 = read.csv("../../data/final_datasets/arrests_2011_nta.csv", stringsAsFactors = T)
```

```{r, message=FALSE, results='hide'}
df_2010$ARREST_DATE = NULL
df_2010$YEAR = NULL
df_2010$Latitude = NULL
df_2010$Longitude = NULL

df_2010$NTA2020 = factor(df_2010$NTA2020)
df_2010$MONTH = factor(df_2010$MONTH)

# add NA category
df_2010$KY_CD = ifelse(is.na(df_2010$KY_CD), "MISSING", df_2010$KY_CD)
df_2010$KY_CD = factor(df_2010$KY_CD)

str(df_2010)

# check for NA
apply(df_2010, 2, function(col) (sum(is.na(col))))

# the ratio is high
nrow(na.omit(df_2010)) / nrow(df_2010)

# delete missing values
df_2010 = na.omit(df_2010)


df_2011$ARREST_DATE = NULL
df_2011$YEAR = NULL
df_2011$Latitude = NULL
df_2011$Longitude = NULL

df_2011$NTA2020 = factor(df_2011$NTA2020)
df_2011$MONTH = NULL

# add NA category
df_2011$KY_CD = ifelse(is.na(df_2011$KY_CD), "MISSING", df_2011$KY_CD)
df_2011$KY_CD = factor(df_2011$KY_CD)

str(df_2011)

# check for NA
apply(df_2011, 2, function(col) (sum(is.na(col))))

# the ratio is high
nrow(na.omit(df_2011)) / nrow(df_2011)

# delete missing values
df_2011 = na.omit(df_2011)
```


## Descrizione

L'obbiettivo di questa sezione di analisi è verificare se esiste un sottoinsieme di variabili esplicative particolarmente correlate con il numero di arresti, sia marginalmente che considerando l'interazione con ciascuna zona spaziale (NTA).
Per vincoli computazionali si riduce l'insieme di stima al solo anno 2010: per quest'anno i dati del censo sono esatti e non si sono verificati eventi rari a differenza del 2020 (Covid); l'insieme di verifica scelto è l'anno 2011, in quanto è l'anno più vicino al 2010 (l'assunzione è che i due anni siano abbastanza simili per il fenomeno considerato).

### Variabili considerate

Viene riportata la lista delle variabili considerate con una breve descrizione.

Le variabili considerate dal dataset arrests sono:

- KY_CD (fattore con 70 livelli): categoria granulare del crimine causa dell'arresto
- LAW_CAT_CD (fattore con 5 livelli): categoriea generale del crimine causa dell'arresto
- AGE_GROUP (fattore con 5 livelli): classe d'età dell'arrestato
- PERP_SEX  (fattore con 2 livelli): sesso dell'arrestato
- PERP_RACE (fattore con 7 livelli): etnia dell'arrestato
- NTA2020 (fattore con 251 livelli): indicatore della specifica NTA del luogo dell'arresto
- MONTH (fattore con 12 livelli): indicatore del mese dell'arresto

Le variabili considerate dal censo sono:
- Pop1 (numerica): popolazione per NTA
- MaleP (numerica): percentuale di maschi per NTA
- MdAge (numerica): età mediana per NTA
- Hsp1P (numerica): percentuale di ispanici per NTA
- WNHP (numerica): percentuale di bianchi non ispanici (NH) per NTA
- BNHP (numerica): percentuale di neri non ispanici per NTA
- ANHP (numerica): percentuale di asiatici non ispanici per NTA
- OthNHP (numerica): percentuale di altre etnie non ispaniche per NTA
- MIncome (numerica): reddito mediano per NTA


### Problematiche

Questi dati presentano diverse problematiche.

Le scelte fatte sono dovute a fattori computazionali, di tempo e al fatto che per permettere conteggi diversi da 1 è necessario considerare zone spaziali e intervalli temporali non eccessivamente ristretti.

Per selezione delle variabili di stratificazione per zona da includere, benchè il censo fornisca molte variabili si è scelto di considerarne solo un esiguo sottoinsieme: ciò è dovuto a una mancanza di conoscenza di campo e al non poter dedicare eccessivo tempo alla selezione ed in particolare all'estrazione delle stesse dai varia database e fogli di calcolo.

Per la definizione delle zone spaziali e degli intervalli temporali.
Inizialmente si era pensato di eseguire le analisi sull'unità spaziale più piccola disponibile per cui sono presenti i dati del censo: i "Census Tracts" (CT) che per New York sono più di 2000 zone distinte (a differenza delle 250 degli NTA), si è però subito verificato che con i mezzi a disposizione lavorare su modelli con i CT era improponibile a causa delle eccessive risorse computazionali richieste. Per quanto concerne gli intervalli temporali si è scelto di ignorare i possibili trend e considerare i singoli anni, per ciascun anno si sono utilizzati i mesi per la costruzione degli insiemi di convalida incrociata. Pur avendo a disposizione il giorno di ciascun arresto la selezione dei mesi è apparsa come un giusto compromesso per garantire dei conteggi superiori a 1.

Pur avendo notevolmente ridotto la potenziale complessità del problema il numero di osservazioni e il numero di modalità dei fattori alcune procedure impiegate possono comunque molto tempo (sulle piattaforme di calcolo impiegate) per essere eseguite: questi limiti rendono proibitive valutazioni dell'incertezza nelle stime, quali metodi bootstrap o bayesiani.

Poichè i conteggi sono costruiti raggruppando le osservazioni con le stesse combinazioni di modalità delle covariate, il conteggio minimo osservabile è uguale a 1.
Ciò pone dei possibili problemi nella stima di modelli che comprendono lo zero nel supporto (ad esempio il modello di Poisson). Si è comunque provato ad adattare modelli (sia a risposta continua che discreta) su questa tipologia di dati, una delle possibili soluzioni al di fuori di impiegare modelli diversi è la metodologia "zero padding" in cui per ogni combinazione di variabili per cui non si verificano casi si aggiunge un'osservazione con conteggio nullo: la criticità qui è computazionale dovuta all'incremento notevole delle osservazioni.


```{r, results='hide'}
year_grouped = suppressMessages(df_2010 %>%
  dplyr::select(-"MONTH") %>% 
  group_by_all() %>% 
  summarise(count = n()))

```



A sinistra i conteggi di arresti raggruppati per valori di covariate e a destra il logaritmo della medesima quantità
```{r, fig.show='hold', out.width = OUT.WIDTH.1d, out.height= OUT.HEIGHT.1d}
par(mfrow = c(1,2))

year_grouped$count %>%
  table %>% 
  plot(main = "Arrests ratio grouped by covariates",
       xlab = "number of arrests",
       ylab = "absolute frequency")

year_grouped$count %>% 
  log %>% 
  table %>% 
  plot(main = " log arrests count grouped by covariates",
       xlab = "log(arrests count)",
       ylab = "absolute frequency")

par(mfrow = c(1,1))
```



```{r}
# to include zeros
# cols_to_complete = colnames(df_2010)
# cols_to_complete = setdiff(cols_to_complete, "MONTH")
# 
# # Create a data frame with all combinations of the columns
# all_combinations <- expand_grid(!!!syms(cols_to_complete))
# 
# # Summarize the data and complete with zeros
# year_grouped_zeros <- df_2010 %>%
#   select(-MONTH) %>%
#   group_by(across(all_of(cols_to_complete))) %>%
#   summarise(count = n(), .groups = 'drop') %>%
#   ungroup() %>%
#   right_join(all_combinations, by = cols_to_complete) %>%
#   replace_na(list(count = 0))
# 
# print(year_grouped_zeros)

```

### Elevata dimensionalità

I dati presentano elevata dimensionalità considerando le interazioni tra la variabile spaziale (NTA) e le covariate (qualitative) di arrests.
E' comunque interessante provare i metodi di selezione delle variabili anche sui dati senza interazioni.

Per avere delle misure quantitative si considera il dataset in cui si sono definiti i conteggi senza considerare i mesi: si riporta il rapporto tra il numero di osservazioni (righe) e il prodotto tra il numero di modalità di NTA e la somma delle modalità delle variabili qualitative di arrests.

```{r}
var_unique_len = apply(year_grouped,
                       2,
                       function(col) length(unique(col)))
```

Senza considerare KY_CD (esplicativa non spaziale di arrest con più modalità) il rapporto è:
```{r, include = TRUE}
nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("LAW_CAT_CD", "AGE_GROUP", "PERP_SEX", "PERP_RACE")]))
```

Considerando anche KY_CD il rapporto è:
```{r, include=TRUE}
nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("KY_CD", "LAW_CAT_CD", "AGE_GROUP",
                       "PERP_SEX", "PERP_RACE")]))
```



```{r}

# Poichè dopo alcune prove le procedure si sono rivelate troppo onerose includendo KY_CD si è deciso di # rimuoverla confidando che l'informazione fornita da LAW_CAT_CD sia comunuque informativa.
# df_2010$KY_CD = NULL
# df_2011$KY_CD = NULL
```

## Modelli

### Criterio di selezione dei parametri di regolazione

Come già accennato, i parametri di regolazione sono selezionati tramite convalida incrociata (CV) impiegando i mesi per la costruzione degli insiemi.

La procedura per la costruzione degli insiemi è la seguente:
- Selezione di k: il numero di insiemi di convalida (ad esempio k = 4)
- Ogni insieme di convalida è composto da osservazioni raggruppate di 12 / k (3) mesi e i mesi rimanenti (9) vengono utilizzati per adattare il modello.
- Per cercare di compensare e mediare le fluttuazioni stagionali, i mesi di validazione sono scelti il più distanziati possibile. Ad esempio, nel caso di k = 4, il primo insieme di validazione è (gennaio, maggio, settembre), il secondo set è (febbraio, giugno, ottobre), il terzo è (marzo, luglio, novembre) e il quarto è (aprile, agosto, dicembre).
- Per rendere ogni risposta comparabile avendo utilizzato un numero diverso di mesi, una nuova risposta è definita come il rapporto degli arresti diviso per il numero di mesi utilizzati nel raggruppamento (ovvero l'esponenziale dell 'offset nel modello di Poisson).


```{r}
# months indexes sets
# each list contains a matrix where each row contains the used indexes
month_sets_ind = list(k4 = matrix(c(1, 5, 9,
                                    2, 6, 10,
                                    3, 7, 11,
                                    4, 8, 12),
                                  byrow = T,
                                  nrow = 4),
                      
                      k6 = matrix(c(1, 7,
                                    2, 8,
                                    3, 9,
                                    4, 10,
                                    5, 11,
                                    6, 12),
                                  byrow = T,
                                  nrow = 6))
```

### Matrice del modello

Le matrici del modello considerate sono quella senza interazioni e quella con interazioni tra NTA e le esplicative in arrests.
```{r, include=TRUE}
FORMULA.NO.INTERACTIONS = as.formula("count ~. -1")

FORMULA.YES.INTERACTIONS = as.formula("count ~. -1 +
                            NTA2020:LAW_CAT_CD + 
                            NTA2020:AGE_GROUP + 
                            NTA2020:PERP_SEX + 
                            NTA2020:PERP_RACE")
```

```{r, warning=FALSE, results='hide'}

df_2010_grouped = suppressMessages(df_2010 %>%
  dplyr::select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n()))

df_2011_grouped = suppressMessages(df_2011 %>%
  group_by_all() %>% 
  summarise(count = n()))

df_2010_grouped.matrix = sparse.model.matrix(FORMULA.YES.INTERACTIONS,
                                                df_2010_grouped)

df_2011_grouped.matrix = sparse.model.matrix(FORMULA.YES.INTERACTIONS,
                                                df_2011_grouped)
gc()
```


Poichè la matrice del modello dell'insieme di verifica e quella dell'insieme di stima non condividono tutte le colonne si sceglie di ridurre le colonne di entrambe con le colonne in comune.
```{r}


# check dimensions
dim(df_2010_grouped.matrix)
dim(df_2011_grouped.matrix)

col_names_2010 = colnames(df_2010_grouped.matrix)
col_names_2011 = colnames(df_2011_grouped.matrix)

common_col_names = intersect(col_names_2010, col_names_2011)

# in order to use 2011 as a test set we need to uniform the 2011 columns to 2010 colums:
# some bias is introduced here, let's hope it's about small counts
df_2010_grouped.matrix = df_2010_grouped.matrix[,common_col_names]
df_2011_grouped.matrix = df_2011_grouped.matrix[,common_col_names]
```

```{r}
# make cv folds

# NO interaction
# month.noint.cv.k4.sets = suppressMessages(MakeMonthCvSets(my_df = df_2010,
#                        month_sets_ind = month_sets_ind$k4,
#                        formula = FORMULA.NO.INTERACTIONS))
# Yes interaction
month.yesint.cv.k4.sets = suppressMessages(MakeMonthCvSets(my_df = df_2010,
                       month_sets_ind = month_sets_ind$k4,
                       formula = FORMULA.YES.INTERACTIONS))
```





### Esplicative quantitative

Benchè le eslplicative quantitative permettano la specificazione di diverse forme funzionali qui, per ragioni computazionali ci si limita ad assumere (forzatamente) una relazione monotona lineare con la risposta.

### Modelli per risposta continua

Nell'impiego dei modelli con risposta continua (con errori gaussiani i.i.d) si è scelto di stimare il modello su una trasformazione logaritmica della risposta: "y = count / n_month_train" e calcolare l'errore di previsioni sulla trasformazione "count = exp(y) * n_month_test" rispetto al numero di conteggi osservati, in questo modo la previsione è sempre positiva.


```{r, eval = bool_execute_heavy_chunks}
models_summary$lasso <- Lasso_CV(month.yesint.cv.k4.sets,
                                        my.lambda.vals = exp(seq(-13, -6, 0.1)) %>% sort(decreasing = T))
```

```{r, fig.show='hold', out.width = OUT.WIDTH.1d, out.height= OUT.HEIGHT.1d}


par(mfrow = c(1,1))
```

```{r, results='hide', ,eval = bool_execute_heavy_chunks}
temp.fit = glmnet(x = df_2010_grouped.matrix,
                  y = log(df_2010_grouped$count) -log(12),
                  lambda = models_summary$lasso$lmin)

models_summary$lasso$beta = temp.fit$beta
models_summary$lasso$test_error = RMSEfun(df_2011_grouped$count, 
                                          exp(predict(temp.fit, newx = df_2011_grouped.matrix) + log(12)))

temp.fit = glmnet(x = df_2010_grouped.matrix,
                  y = log(df_2010_grouped$count) -log(12),
                  lambda = models_summary$lasso$l1se)

models_summary$lasso$beta1se = temp.fit$beta
models_summary$lasso$test_error1se = RMSEfun(df_2011_grouped$count, 
                                          exp(predict(temp.fit, newx = df_2011_grouped.matrix) + log(12)))


```


```{r, eval = bool_execute_heavy_chunks}

models_summary$elasticnet = Elastic_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-20, -8, 0.1)) %>% sort(decreasing = TRUE),
                                             my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20))
```

```{r, fig.show='hold', out.width = OUT.WIDTH, out.height= OUT.HEIGHT}

TwoParErrPlot(model_list = models_summary$elasticnet,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

par(mfrow = c(1,1))
```

```{r, eval = bool_execute_heavy_chunks}
temp.fit = glmnet(x = df_2010_grouped.matrix,
                  y = log(df_2010_grouped$count) -log(12),
                  alpha = models_summary$elasticnet$amin,
                  lambda = models_summary$elasticnet$lmin)

models_summary$elasticnet$beta = temp.fit$beta
models_summary$elasticnet$test_error = RMSEfun(df_2011_grouped$count, 
                                          exp(predict(temp.fit, newx = df_2011_grouped.matrix) + log(12)))
```


```{r}
# A = colnames(month.noint.cv.k4.sets[[1]]$train$df)
# # Create a named vector to map prefixes to values
# prefix_to_value <- setNames(seq_along(A), A) 
# 
# B = colnames(month.noint.cv.k4.sets[[1]]$train$model_matrix)
# 
# # Create indexes vector
# var_indexes <- sapply(B, function(item) {
#   prefix <- A[sapply(A, function(prefix) startsWith(item, prefix))]
#   prefix_to_value[prefix]
# })

```


```{r}
# models_summary$glasso = GLasso_CV(month.noint.cv.k4.sets,
#                                         my.lambda.vals = exp(seq(-17, -6, 0.1)) %>% sort(decreasing = T),
#                                   group_index = var_indexes)
```


```{r, warning = FALSE, eval = bool_execute_heavy_chunks}
models_summary$scad = NonConvex_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-5, 1, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 10, length = 10),
                                         my.penalty = "scad")

```

```{r, fig.show='hold', out.width = OUT.WIDTH, out.height= OUT.HEIGHT}



```

```{r,  warning=FALSE, eval = bool_execute_heavy_chunks}
# temp.fit = ncvreg(x = as.matrix(df_2010_grouped.matrix),
#                  y = log(df_2010_grouped$count) -log(12),
#                  gamma = models_summary$scad$gmin,
#                  penalty = "SCAD")
# 
# plot(temp.fit, log.l = TRUE)
# abline(v = log(models_summary$scad$lmin), col = "blue", lty = 2, lwd = 2)
# 
# models_summary$scad$beta = temp.fit$beta
# models_summary$scad$test_error = RMSEfun(df_2011_grouped$count, 
#                                           exp(predict(temp.fit, X = df_2011_grouped.matrix) + log(12)))
```


```{r, warning = FALSE, eval = bool_execute_heavy_chunks}
models_summary$mcp = NonConvex_Cv(cv.sets = month.yesint.cv.k4.sets,
                                             my.lambda.vals = exp(seq(-5, 5, 0.2)) %>% sort(decreasing = TRUE),
                                             my.gamma.vals = seq(2+1e-05, 10, length = 10),
                                         my.penalty = "mcp")
```


```{r, fig.show='hold', out.width = OUT.WIDTH, out.height= OUT.HEIGHT}



par(mfrow = c(1,1))
```

```{r, eval = bool_execute_heavy_chunks}
# temp.fit = ncvreg(x = as.matrix(df_2010_grouped.matrix),
#                  y = log(df_2010_grouped$count) -log(12),
#                  gamma = models_summary$mcp$gmin,
#                  penalty = "MCP")
# 
# plot(temp.fit, log.l = TRUE)
# abline(v = log(models_summary$mcp$lmin), col = "blue", lty = 2, lwd = 2)
# 
# models_summary$mcp$beta = temp.fit$beta
# models_summary$mcp$test_error = RMSEfun(df_2011_grouped$count, 
#                                           exp(predict(temp.fit, X = df_2011_grouped.matrix) + log(12)))



```

```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```


```{r}
# Known convergence difficult, see help here. https://cran.r-project.org/web/packages/glmnet/vignettes/glmnetFamily.pdf
glmnet.control(mxitnr = 50) # increase maximum no. of IRLS iterations allowed
```

```{r, eval = bool_execute_heavy_chunks, warning=FALSE}
# models_summary$lasso.poi.noint = Lasso_Offset_CV(cv.sets = month.noint.cv.k4.sets,
#                                           my.lambda.vals = exp(seq(-15, 3, 0.1)) %>% sort(decreasing = TRUE),
#                                           my.family = poisson())
```

```{r, eval = bool_execute_heavy_chunks, warning=FALSE}
models_summary$lasso.poi = Lasso_Offset_CV(cv.sets = month.yesint.cv.k4.sets,
                                          my.lambda.vals = exp(seq(-15, 3, 0.1)) %>% sort(decreasing = TRUE),
                                          my.family = poisson())
```

```{r, fig.show='hold', out.width = OUT.WIDTH.1d, out.height= OUT.HEIGHT.1d}
par(mfrow = c(1,2))

PlotOneDim(x = log(models_summary$lasso$lambda),
           y = models_summary$lasso$cv.err.matr$cv.err,
           se = models_summary$lasso$cv.err.matr$cv.se,
           x.min = log(models_summary$lasso$lmin),
           x.1se = log(models_summary$lasso$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO yes interaction CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

PlotOneDim(x = log(models_summary$lasso.poi$lambda),
           y = models_summary$lasso.poi$cv.err.matr$cv.err,
           se = models_summary$lasso.poi$cv.err.matr$cv.se,
           x.min = log(models_summary$lasso.poi$lmin),
           x.1se = log(models_summary$lasso.poi$l1se),
           xlab = "log lambda",
           ylab = "CV error",
           main = "LASSO Poisson yes interaction CV error",
          min.leg = "lambda min",
           onese.leg = "lambda 1se")

par(mfrow = c(1,1))
```


```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```


```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
# models_summary$elasticnet.poi.noint = Elastic_Offset_Cv(cv.sets = month.noint.cv.k4.sets,
#                                                         my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
#                                                         my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20),
#                                                         my.family = poisson())
```

```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
models_summary$elasticnet.poi = Elastic_Offset_Cv(cv.sets = month.yesint.cv.k4.sets,
                                                        my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
                                                        my.alpha.vals = seq(1e-5, 1 - 1e-5, length = 20),
                                                        my.family = poisson())
```

```{r, fig.show='hold', out.width = OUT.WIDTH, out.height= OUT.HEIGHT}

TwoParErrPlot(model_list = models_summary$elasticnet,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

TwoParErrPlot(model_list = models_summary$mcp,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "MCP yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

TwoParErrPlot(model_list = models_summary$scad,
              row_par_name = "lambda",
              col_par_name = "gamma",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "gmin",
              my.main = "SCAD yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "gamma")

TwoParErrPlot(model_list = models_summary$elasticnet.poi,
              row_par_name = "lambda",
              col_par_name = "alpha",
              cv_err_matr_name = "cv.err.matr",
              row_par_min_name = "lmin",
              col_par_min_name = "amin",
              my.main = "Elasticnet Poisson yes interaction Cv error contour",
              my.xlab = "log lambda",
              my.ylab = "alpha")

par(mfrow = c(1,1))
```

```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```




```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
### Neg bin Lasso

# Using a hierarchical specification we assume $Y_i \sim P(\mu_i \lambda_i)$ and $\lambda_i \sim Ga(\tau, \tau)$ so marginally the $Y_i$ are negative binomials with variance $\mu_i(1 + \tau \mu_i)$.
# In the R parameterization adopted $\theta = 1/\tau$ which becomes another tuning parameter.

# models_summary$lasso.negbin.noint = LASSO_NegBin_Offset_Cv(cv.sets = month.noint.cv.k4.sets,
#                                                         my.lambda.vals = exp(seq(-3, 5, 0.2)) %>% sort(decreasing = TRUE),
#                                                         my.theta.vals = seq(1e-5, 3, length = 20))
```

```{r, warning=FALSE, eval = bool_execute_heavy_chunks}
# models_summary$lasso.negbin = LASSO_NegBin_Offset_Cv(cv.sets = month.yesint.cv.k4.sets,
#                                                         my.lambda.vals = exp(seq(-10, 2, 0.2)) %>% sort(decreasing = TRUE),
#                                                         my.theta.vals = seq(1e-5, 20, length = 20))
```

```{r, fig.show='hold', out.width = OUT.WIDTH, out.height= OUT.HEIGHT}
par(mfrow = c(1,1))
# 
# TwoParErrPlot(model_list = models_summary$lasso.negbin.noint,
#               row_par_name = "lambda",
#               col_par_name = "theta",
#               cv_err_matr_name = "cv.err.matr",
#               row_par_min_name = "lmin",
#               col_par_min_name = "tmin",
#               my.main = "LASSO Negative Binomial no interaction Cv error contour",
#               my.xlab = "log lambda",
#               my.ylab = "theta")

# TwoParErrPlot(model_list = models_summary$lasso.negbin,
#               row_par_name = "lambda",
#               col_par_name = "theta",
#               cv_err_matr_name = "cv.err.matr",
#               row_par_min_name = "lmin",
#               col_par_min_name = "tmin",
#               my.main = "LASSO Negative Binomial yes interaction Cv error contour",
#               my.xlab = "log lambda",
#               my.ylab = "theta")

par(mfrow = c(1,1))
```

```{r}
# backup to save key model info
save(models_summary, file = file_save_name)
```

### Modelli migliori

```{r}
extracted_best_pars  = list()

for (model in names(models_summary)){
  extracted_best_pars[[model]] = ExtractBestPars(models_summary[[model]])
}

```











