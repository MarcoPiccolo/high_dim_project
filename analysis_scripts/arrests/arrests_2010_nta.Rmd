---
title: "Arrests 2010 NTA analysis"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
```

```{r, message=FALSE, warning=FALSE}
rm(list = ls())
gc()

library(tidyverse)
library(Matrix)
```
## Preprocessing
```{r}
# read the data
my.df = read.csv("../../data/core_datasets/arrests/arrests_2010_nta.csv", stringsAsFactors = T)
```

Remove date and NTA variables, convert to factor location, month and KY_CD.
```{r}
my.df$ARREST_DATE = NULL

my.df$NTA2020 = factor(my.df$NTA2020)
my.df$MONTH = factor(my.df$MONTH)

# add NA category
my.df$KY_CD = ifelse(is.na(my.df$KY_CD), "MISSING", my.df$KY_CD)
my.df$KY_CD = factor(my.df$KY_CD)
```


```{r}
str(my.df)
```

Check for NA
```{r}
apply(my.df, 2, function(col) (sum(is.na(col))))
```


A possibility is to get rid of all NAs rows, the portion of deleted rows would be relatively small (of course we're introducing some bias here).
```{r}
nrow(na.omit(my.df)) / nrow(my.df)
```
Other possibilities would be to impute values for numerical variables (using median, mean or more sofisticated methods).
For simplicity we just delete missing values rows.
```{r}
my.df = na.omit(my.df)
```


## Description
Ideally 2010 data are our training set and 2011 data are the test set.
The goal of the analysis is to identify is some covariates are correlated with the arrests rate: more specifically if the response is well explained by some non spatial covariates alone, some spatial alone or interaction between the two.

A reasonable response variable would be the count of arrests divided by the local (space zone) population, also grouping by any other covariates value.



To get an idea of the dataset used on which models are tested a
```{r}
year_grouped = my.df %>%
  select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n())

# define the arrest rate (count / population)
year_grouped$y =  year_grouped$count / year_grouped$Pop1

```

```{r}
dim(year_grouped)
colnames(year_grouped)
```
Still a huge number of observations compared to the number of variables, but what if we add interactions?

Let's look at the distribution of the counts
```{r}
year_grouped$count %>%
  table %>% 
  plot(main = "Arrests counts grouped by covariates",
       xlab = "arrest number",
       ylab = "absolute frequency")
```
We can see an inflation of ones.
The ratios present a similar table.
```{r}
year_grouped$y %>%
  table %>% 
  plot(main = "Arrests ratio grouped by covariates",
       xlab = "number of arrests / census tract population",
       ylab = "absolute frequency")
```


Let's count the hipothetical number of interaction terms if ones considers only interactions between spatial zones and selected arrests covariates along with the obervations /  number of parameter ratio (underestimate since there are other variables):
```{r}
var_unique_len = apply(year_grouped,
                       2,
                       function(col) length(unique(col)))

var_unique_len
```

Not including KY_CD:
```{r}
var_unique_len["NTA2020"] *
  sum(var_unique_len[c("LAW_CAT_CD", "AGE_GROUP", "PERP_SEX", "PERP_RACE")])

nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("LAW_CAT_CD", "AGE_GROUP", "PERP_SEX", "PERP_RACE")]))
```

Including KY_CD
```{r}
var_unique_len["NTA2020"] *
  sum(var_unique_len[c("KY_CD","LAW_CAT_CD", "AGE_GROUP",
                       "PERP_SEX", "PERP_RACE")])

nrow(year_grouped) / (var_unique_len["NTA2020"] *
  sum(var_unique_len[c("KY_CD", "LAW_CAT_CD", "AGE_GROUP",
                       "PERP_SEX", "PERP_RACE")]))
```
We decide to not employ the MONTH time variable as a covariate but use it for a model selection method.


### Variables description

Original dataset selected variables:

Census stratification variables:


## Explorative analysis


## Models

### Model selection method

Given the previously described constraints, in order to be able to apply a cross validation (CV) selection method we choose to ignore the time (MONTH) factor using MONTH as index to create the CV folds as described below.
Choose k: the number of validation sets (example k = 4) each validation set is made by grouped observations of 12 / k (3) months and the months left are used to fit the model. To try to compensate and average for seasonal fluctuations the validation months are chosen as spaced as possible, for example, in the case k = 4 the first validation set is (january, may, september), the second set is (february, june, october), the third is (march, july, november) and the forth is (april, august, december); in order to make each response comparable having used a different number of months a new response is defined as the arrests ratio divided by the number of months used in the grouping.

```{r}
# Groub by months indexes
GroupByMONTHSets = function(mydf, month_indexes){
  month_grouped = mydf %>%
    filter(MONTH %in% month_indexes) %>% 
    select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n())
  
  month_grouped$y = month_grouped$count /
    (month_grouped$Pop1 * length(month_indexes))
  
  return(month_grouped)
}
```

Define Month indexes
```{r}
# months indexes sets
# each list contains a matrix where each row contains the used indexes
month_sets_ind = list(k4 = matrix(c(1, 5, 9,
                                    2, 6, 10,
                                    3, 7, 11,
                                    4, 8, 12),
                                  byrow = T,
                                  nrow = 4),
                      k6 = matrix(c(1, 7,
                                    2, 8,
                                    3, 9,
                                    4, 10,
                                    5, 11,
                                    6, 12),
                                  byrow = T,
                                  nrow = 6))
```

In order to simplify computations we remove the KY_CD variable (hoping LAW_CAT_CD will be sufficient to describe the crime tipe)

```{r}
my.df$KY_CD = NULL

my.df.year.grouped = my.df %>%
  select(-MONTH) %>% 
  group_by_all() %>% 
  summarise(count = n())

my.df.year.grouped$y = my.df.year.grouped$count / my.df.year.grouped$Pop1
```


```{r}
FORMULA.NO.KY_CD = as.formula("y ~. -1 -Pop1 - count + 
                            NTA2020:LAW_CAT_CD +
                            NTA2020:AGE_GROUP +
                            NTA2020:PERP_SEX +
                            NTA2020:PERP_RACE")
```

Error functions
```{r}
RMSEfun = function(true_vals, pred_vals){
  sqrt(sum( (true_vals - pred_vals)^2 ))
}
```



Model matrices: omit KY_CD variables due to computational issues.
Make a list of fit and validation sets:

```{r, warning=FALSE, message=FALSE}

month.fit.val.k4.sets = list()

for (i in 1:nrow(month_sets_ind$k4)){
  
  temp.df.train = GroupByMONTHSets(mydf = my.df,
                       month_indexes = setdiff(1:12, month_sets_ind$k4[i,]))
  
  temp.df.test = GroupByMONTHSets(mydf = my.df,
                       month_indexes = month_sets_ind$k4[i,])
  
  # make sublists
  month.fit.val.k4.sets[[i]] = list(train = list(y = NA,model_matrix = NA),
                                    test = list(y = NA,model_matrix = NA))
  
  # populate the sublists
  month.fit.val.k4.sets[[i]]$train$y = temp.df.train$y
  month.fit.val.k4.sets[[i]]$train$model_matrix = sparse.model.matrix(FORMULA.NO.KY_CD,
                          data = temp.df.train)
  
  month.fit.val.k4.sets[[i]]$test$y = temp.df.test$y
  month.fit.val.k4.sets[[i]]$test$model_matrix = sparse.model.matrix(FORMULA.NO.KY_CD,
                          data = temp.df.test)
  gc()
}

```

### Note on quantitative covariates

The simplest assumption is to assume a linear (monotone) trend of the response as a function of quantitative covariates.

### LASSO

```{r}
library(glmnet)

# first compute a lambda grid on all grouped dataset

all.fit = glmnet(x = sparse.model.matrix(FORMULA.NO.KY_CD, data = my.df.year.grouped),
                 y = my.df.year.grouped$y,
                 alpha = 1)

# validation error matrix
# each col
lasso.cv.error.matr = matrix(NA,
                             nrow = length(all.fit$lambda),
                             ncol = nrow(month_sets_ind$k4))


for (i in 1:dim(lasso.cv.error.matr)[2]){
  temp.lasso = glmnet(x = month.fit.val.k4.sets[[i]]$train$model_matrix,
                 y = month.fit.val.k4.sets[[i]]$train$y,
                 alpha = 1,
                 lambda = all.fit$lambda)
  
  pred.lasso = predict.glmnet(temp.lasso,
                              newx = month.fit.val.k4.sets[[i]]$test$model_matrix)
  
  lasso.cv.error.matr[,i] = apply(pred.lasso,
                                  2,
                                  function(col) RMSEfun(true_vals = month.fit.val.k4.sets[[i]]$test$y,
                                                        pred_vals = col))
  
  rm(temp.lasso)
  rm(pred.lasso)
  gc()
  
}

```

```{r}

plot(log(all.fit$lambda), apply(lasso.cv.error.matr, 1, mean),
     xlab = "log lambda",
     ylab = "RMSE",
     main = "LASSO CV error",
     xlim = c(-13, -10),
     ylim = c(0.23, 0.25),
     pch = 16,
     type = "b")
```



### Elasticnet

### Grouped LASSO

### Scad MCP











